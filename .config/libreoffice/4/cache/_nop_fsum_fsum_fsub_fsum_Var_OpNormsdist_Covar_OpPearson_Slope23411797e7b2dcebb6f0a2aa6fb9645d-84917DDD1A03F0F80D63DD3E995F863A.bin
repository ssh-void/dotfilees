//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-34957518
// Unknown Toolkit Version
// Based on NVVM 7.0.1
//

.version 8.4
.target sm_50, texmode_independent
.address_size 64

	// .globl	DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope

.entry DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope(
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_0,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_1,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_2,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_3,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_4,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_5,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_6,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_7,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_8,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_9
)
{
	.reg .pred 	%p<356>;
	.reg .f32 	%f<13>;
	.reg .b32 	%r<298>;
	.reg .f64 	%fd<1566>;
	.reg .b64 	%rd<70>;


	ld.param.u64 	%rd17, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_1];
	ld.param.u64 	%rd18, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_2];
	ld.param.u64 	%rd19, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_3];
	ld.param.u64 	%rd20, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_4];
	ld.param.u64 	%rd21, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_5];
	ld.param.u64 	%rd22, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_6];
	ld.param.u64 	%rd23, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_7];
	ld.param.u64 	%rd24, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_8];
	ld.param.u64 	%rd25, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_9];
	mov.b32 	%r81, %envreg3;
	mov.u32 	%r82, %ctaid.x;
	mov.u32 	%r83, %ntid.x;
	mov.u32 	%r84, %tid.x;
	add.s32 	%r85, %r84, %r81;
	mad.lo.s32 	%r1, %r83, %r82, %r85;
	cvt.s64.s32 	%rd1, %r1;
	setp.gt.s32 	%p1, %r1, 5;
	mov.f64 	%fd1488, 0d0000000000000000;
	mov.f64 	%fd1489, %fd1488;
	mov.f64 	%fd1490, %fd1488;
	@%p1 bra 	$L__BB0_6;

	mov.f64 	%fd1490, 0d0000000000000000;
	mov.u32 	%r271, 1;
	mov.u32 	%r270, %r1;
	mov.f64 	%fd1489, %fd1490;
	mov.f64 	%fd1488, %fd1490;

$L__BB0_2:
	mul.wide.s32 	%rd26, %r270, 8;
	add.s64 	%rd27, %rd24, %rd26;
	add.s64 	%rd28, %rd25, %rd26;
	ld.global.f64 	%fd4, [%rd28];
	ld.global.f64 	%fd5, [%rd27];
	abs.f64 	%fd249, %fd5;
	setp.gtu.f64 	%p2, %fd249, 0d7FF0000000000000;
	@%p2 bra 	$L__BB0_5;

	abs.f64 	%fd250, %fd4;
	setp.gtu.f64 	%p3, %fd250, 0d7FF0000000000000;
	@%p3 bra 	$L__BB0_5;

	add.f64 	%fd1490, %fd1490, %fd4;
	add.f64 	%fd1489, %fd1489, %fd5;
	add.f64 	%fd1488, %fd1488, 0d3FF0000000000000;

$L__BB0_5:
	add.s32 	%r270, %r271, %r1;
	setp.lt.s32 	%p4, %r270, 6;
	setp.lt.u32 	%p5, %r271, 2;
	mov.u32 	%r271, 2;
	and.pred  	%p6, %p5, %p4;
	@%p6 bra 	$L__BB0_2;

$L__BB0_6:
	setp.lt.f64 	%p7, %fd1488, 0d3FF0000000000000;
	mov.f64 	%fd1500, 0d7FF8000000000207;
	@%p7 bra 	$L__BB0_15;

	div.rn.f64 	%fd15, %fd1490, %fd1488;
	div.rn.f64 	%fd16, %fd1489, %fd1488;
	mov.f64 	%fd1496, 0d0000000000000000;
	mov.f64 	%fd1497, %fd1496;
	@%p1 bra 	$L__BB0_13;

	mov.f64 	%fd1497, 0d0000000000000000;
	mov.u32 	%r273, 1;
	mov.u32 	%r272, %r1;
	mov.f64 	%fd1496, %fd1497;

$L__BB0_9:
	mul.wide.s32 	%rd29, %r272, 8;
	add.s64 	%rd30, %rd24, %rd29;
	add.s64 	%rd31, %rd25, %rd29;
	ld.global.f64 	%fd19, [%rd31];
	ld.global.f64 	%fd20, [%rd30];
	abs.f64 	%fd256, %fd20;
	setp.gtu.f64 	%p9, %fd256, 0d7FF0000000000000;
	@%p9 bra 	$L__BB0_12;

	abs.f64 	%fd257, %fd19;
	setp.gtu.f64 	%p10, %fd257, 0d7FF0000000000000;
	@%p10 bra 	$L__BB0_12;

	sub.f64 	%fd258, %fd19, %fd15;
	sub.f64 	%fd259, %fd20, %fd16;
	fma.rn.f64 	%fd1497, %fd258, %fd259, %fd1497;
	fma.rn.f64 	%fd1496, %fd258, %fd258, %fd1496;

$L__BB0_12:
	add.s32 	%r272, %r273, %r1;
	setp.lt.s32 	%p11, %r272, 6;
	setp.lt.u32 	%p12, %r273, 2;
	mov.u32 	%r273, 2;
	and.pred  	%p13, %p12, %p11;
	@%p13 bra 	$L__BB0_9;

$L__BB0_13:
	setp.eq.f64 	%p14, %fd1496, 0d0000000000000000;
	mov.f64 	%fd1500, 0d7FF8000000000214;
	@%p14 bra 	$L__BB0_15;

	div.rn.f64 	%fd1500, %fd1497, %fd1496;

$L__BB0_15:
	add.f64 	%fd29, %fd1500, 0d0000000000000000;
	mov.f64 	%fd1504, 0d0000000000000000;
	mov.f64 	%fd1505, %fd1504;
	mov.f64 	%fd1506, %fd1504;
	@%p1 bra 	$L__BB0_21;

	mov.f64 	%fd1506, 0d0000000000000000;
	mov.u32 	%r275, 1;
	mov.u32 	%r274, %r1;
	mov.f64 	%fd1505, %fd1506;
	mov.f64 	%fd1504, %fd1506;

$L__BB0_17:
	mul.wide.s32 	%rd32, %r274, 8;
	add.s64 	%rd33, %rd22, %rd32;
	add.s64 	%rd34, %rd23, %rd32;
	ld.global.f64 	%fd33, [%rd34];
	ld.global.f64 	%fd34, [%rd33];
	abs.f64 	%fd267, %fd34;
	setp.gtu.f64 	%p16, %fd267, 0d7FF0000000000000;
	@%p16 bra 	$L__BB0_20;

	abs.f64 	%fd268, %fd33;
	setp.gtu.f64 	%p17, %fd268, 0d7FF0000000000000;
	@%p17 bra 	$L__BB0_20;

	add.f64 	%fd1506, %fd1506, %fd33;
	add.f64 	%fd1505, %fd1505, %fd34;
	add.f64 	%fd1504, %fd1504, 0d3FF0000000000000;

$L__BB0_20:
	add.s32 	%r274, %r275, %r1;
	setp.lt.s32 	%p18, %r274, 6;
	setp.lt.u32 	%p19, %r275, 2;
	mov.u32 	%r275, 2;
	and.pred  	%p20, %p19, %p18;
	@%p20 bra 	$L__BB0_17;

$L__BB0_21:
	setp.lt.f64 	%p21, %fd1504, 0d3FF0000000000000;
	mov.f64 	%fd1519, 0d7FF8000000000207;
	@%p21 bra 	$L__BB0_30;

	div.rn.f64 	%fd44, %fd1506, %fd1504;
	div.rn.f64 	%fd45, %fd1505, %fd1504;
	mov.f64 	%fd1513, 0d0000000000000000;
	mov.f64 	%fd1514, %fd1513;
	mov.f64 	%fd1515, %fd1513;
	@%p1 bra 	$L__BB0_28;

	mov.f64 	%fd1515, 0d0000000000000000;
	mov.u32 	%r277, 1;
	mov.u32 	%r276, %r1;
	mov.f64 	%fd1514, %fd1515;
	mov.f64 	%fd1513, %fd1515;

$L__BB0_24:
	mul.wide.s32 	%rd35, %r276, 8;
	add.s64 	%rd36, %rd22, %rd35;
	add.s64 	%rd37, %rd23, %rd35;
	ld.global.f64 	%fd49, [%rd37];
	ld.global.f64 	%fd50, [%rd36];
	abs.f64 	%fd276, %fd50;
	setp.gtu.f64 	%p23, %fd276, 0d7FF0000000000000;
	@%p23 bra 	$L__BB0_27;

	abs.f64 	%fd277, %fd49;
	setp.gtu.f64 	%p24, %fd277, 0d7FF0000000000000;
	@%p24 bra 	$L__BB0_27;

	sub.f64 	%fd278, %fd49, %fd44;
	sub.f64 	%fd279, %fd50, %fd45;
	fma.rn.f64 	%fd1515, %fd278, %fd279, %fd1515;
	fma.rn.f64 	%fd1514, %fd278, %fd278, %fd1514;
	fma.rn.f64 	%fd1513, %fd279, %fd279, %fd1513;

$L__BB0_27:
	add.s32 	%r276, %r277, %r1;
	setp.lt.s32 	%p25, %r276, 6;
	setp.lt.u32 	%p26, %r277, 2;
	mov.u32 	%r277, 2;
	and.pred  	%p27, %p26, %p25;
	@%p27 bra 	$L__BB0_24;

$L__BB0_28:
	setp.eq.f64 	%p28, %fd1513, 0d0000000000000000;
	setp.eq.f64 	%p29, %fd1514, 0d0000000000000000;
	or.pred  	%p30, %p28, %p29;
	mov.f64 	%fd1519, 0d7FF8000000000214;
	@%p30 bra 	$L__BB0_30;

	mul.f64 	%fd281, %fd1513, %fd1514;
	sqrt.rn.f64 	%fd282, %fd281;
	div.rn.f64 	%fd1519, %fd1515, %fd282;

$L__BB0_30:
	add.f64 	%fd62, %fd1519, 0d0000000000000000;
	mov.f64 	%fd1523, 0d0000000000000000;
	mov.f64 	%fd1524, %fd1523;
	mov.f64 	%fd1525, %fd1523;
	@%p1 bra 	$L__BB0_36;

	mov.f64 	%fd1525, 0d0000000000000000;
	mov.u32 	%r279, 1;
	mov.u32 	%r278, %r1;
	mov.f64 	%fd1524, %fd1525;
	mov.f64 	%fd1523, %fd1525;

$L__BB0_32:
	mul.wide.s32 	%rd38, %r278, 8;
	add.s64 	%rd39, %rd20, %rd38;
	add.s64 	%rd40, %rd21, %rd38;
	ld.global.f64 	%fd66, [%rd40];
	ld.global.f64 	%fd67, [%rd39];
	abs.f64 	%fd289, %fd67;
	setp.gtu.f64 	%p32, %fd289, 0d7FF0000000000000;
	@%p32 bra 	$L__BB0_35;

	abs.f64 	%fd290, %fd66;
	setp.gtu.f64 	%p33, %fd290, 0d7FF0000000000000;
	@%p33 bra 	$L__BB0_35;

	add.f64 	%fd1525, %fd1525, %fd67;
	add.f64 	%fd1524, %fd1524, %fd66;
	add.f64 	%fd1523, %fd1523, 0d3FF0000000000000;

$L__BB0_35:
	add.s32 	%r278, %r279, %r1;
	setp.lt.s32 	%p34, %r278, 6;
	setp.lt.u32 	%p35, %r279, 2;
	mov.u32 	%r279, 2;
	and.pred  	%p36, %p35, %p34;
	@%p36 bra 	$L__BB0_32;

$L__BB0_36:
	setp.lt.f64 	%p37, %fd1523, 0d3FF0000000000000;
	mov.f64 	%fd1532, 0d7FF8000000000207;
	@%p37 bra 	$L__BB0_44;

	mov.f64 	%fd1530, 0d0000000000000000;
	@%p1 bra 	$L__BB0_43;

	div.rn.f64 	%fd77, %fd1524, %fd1523;
	div.rn.f64 	%fd78, %fd1525, %fd1523;
	mov.f64 	%fd1530, 0d0000000000000000;
	mov.u32 	%r281, 1;
	mov.u32 	%r280, %r1;

$L__BB0_39:
	mul.wide.s32 	%rd41, %r280, 8;
	add.s64 	%rd42, %rd20, %rd41;
	add.s64 	%rd43, %rd21, %rd41;
	ld.global.f64 	%fd80, [%rd43];
	ld.global.f64 	%fd81, [%rd42];
	abs.f64 	%fd294, %fd81;
	setp.gtu.f64 	%p39, %fd294, 0d7FF0000000000000;
	@%p39 bra 	$L__BB0_42;

	abs.f64 	%fd295, %fd80;
	setp.gtu.f64 	%p40, %fd295, 0d7FF0000000000000;
	@%p40 bra 	$L__BB0_42;

	sub.f64 	%fd296, %fd81, %fd78;
	sub.f64 	%fd297, %fd80, %fd77;
	fma.rn.f64 	%fd1530, %fd296, %fd297, %fd1530;

$L__BB0_42:
	add.s32 	%r280, %r281, %r1;
	setp.lt.s32 	%p41, %r280, 6;
	setp.lt.u32 	%p42, %r281, 2;
	mov.u32 	%r281, 2;
	and.pred  	%p43, %p42, %p41;
	@%p43 bra 	$L__BB0_39;

$L__BB0_43:
	div.rn.f64 	%fd1532, %fd1530, %fd1523;

$L__BB0_44:
	setp.gt.s32 	%p44, %r1, 4;
	mov.f64 	%fd1533, 0d7FFFFFFFE0000000;
	@%p44 bra 	$L__BB0_46;

	shl.b64 	%rd44, %rd1, 3;
	add.s64 	%rd45, %rd19, %rd44;
	ld.global.f64 	%fd1533, [%rd45];

$L__BB0_46:
	abs.f64 	%fd300, %fd1533;
	setp.le.f64 	%p45, %fd300, 0d7FF0000000000000;
	mul.f64 	%fd301, %fd1533, 0dBFE6A09E667F3BCC;
	selp.f64 	%fd299, %fd301, 0d8000000000000000, %p45;
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r98}, %fd299; 
	}
	// end inline asm
	setp.lt.s32 	%p46, %r98, 1072168960;
	@%p46 bra 	$L__BB0_51;
	bra.uni 	$L__BB0_47;

$L__BB0_51:
	abs.f64 	%fd548, %fd299;
	mov.f64 	%fd549, 0d3D47088FDB46FA5F;
	mov.f64 	%fd550, 0dBCF0679AFBA6F279;
	fma.rn.f64 	%fd551, %fd550, %fd548, %fd549;
	mov.f64 	%fd552, 0dBD8DF9F9B976A9B2;
	fma.rn.f64 	%fd553, %fd551, %fd548, %fd552;
	mov.f64 	%fd554, 0d3DC7F1F5590CC332;
	fma.rn.f64 	%fd555, %fd553, %fd548, %fd554;
	mov.f64 	%fd556, 0dBDFA28A3CD2D56C4;
	fma.rn.f64 	%fd557, %fd555, %fd548, %fd556;
	mov.f64 	%fd558, 0d3E2485EE67835925;
	fma.rn.f64 	%fd559, %fd557, %fd548, %fd558;
	mov.f64 	%fd560, 0dBE476DB45919F583;
	fma.rn.f64 	%fd561, %fd559, %fd548, %fd560;
	mov.f64 	%fd562, 0d3E62D698D98C8D71;
	fma.rn.f64 	%fd563, %fd561, %fd548, %fd562;
	mov.f64 	%fd564, 0dBE720A2C7155D5C6;
	fma.rn.f64 	%fd565, %fd563, %fd548, %fd564;
	mov.f64 	%fd566, 0dBE41D29B37CA1397;
	fma.rn.f64 	%fd567, %fd565, %fd548, %fd566;
	mov.f64 	%fd568, 0d3EA2EF6CC0F67A49;
	fma.rn.f64 	%fd569, %fd567, %fd548, %fd568;
	mov.f64 	%fd570, 0dBEC102B892333B6F;
	fma.rn.f64 	%fd571, %fd569, %fd548, %fd570;
	mov.f64 	%fd572, 0d3ECA30375BA9A84E;
	fma.rn.f64 	%fd573, %fd571, %fd548, %fd572;
	mov.f64 	%fd574, 0d3ECAAD18DEDEA43E;
	fma.rn.f64 	%fd575, %fd573, %fd548, %fd574;
	mov.f64 	%fd576, 0dBEFF05355BC5B225;
	fma.rn.f64 	%fd577, %fd575, %fd548, %fd576;
	mov.f64 	%fd578, 0d3F10E37A3108BC8B;
	fma.rn.f64 	%fd579, %fd577, %fd548, %fd578;
	mov.f64 	%fd580, 0d3EFB292D828E5CB2;
	fma.rn.f64 	%fd581, %fd579, %fd548, %fd580;
	mov.f64 	%fd582, 0dBF4356626EBF9BFA;
	fma.rn.f64 	%fd583, %fd581, %fd548, %fd582;
	mov.f64 	%fd584, 0d3F5BCA68F73D6AFC;
	fma.rn.f64 	%fd585, %fd583, %fd548, %fd584;
	mov.f64 	%fd586, 0dBF2B6B69EBBC280B;
	fma.rn.f64 	%fd587, %fd585, %fd548, %fd586;
	mov.f64 	%fd588, 0dBF9396685912A453;
	fma.rn.f64 	%fd589, %fd587, %fd548, %fd588;
	mov.f64 	%fd590, 0d3FBA4F4E2A1ABEF8;
	fma.rn.f64 	%fd591, %fd589, %fd548, %fd590;
	mov.f64 	%fd592, 0d3FE45F306DC9C8BB;
	fma.rn.f64 	%fd593, %fd591, %fd548, %fd592;
	mov.f64 	%fd594, 0d3FC06EBA8214DB69;
	fma.rn.f64 	%fd595, %fd593, %fd548, %fd594;
	fma.rn.f64 	%fd596, %fd595, %fd548, %fd548;
	sub.f64 	%fd597, %fd548, %fd596;
	fma.rn.f64 	%fd598, %fd595, %fd548, %fd597;
	neg.f64 	%fd599, %fd596;
	neg.f64 	%fd600, %fd598;
	cvt.rn.f32.f64 	%f5, %fd596;
	mul.f32 	%f6, %f5, 0fBFB8AA3B;
	cvt.rni.f32.f32 	%f7, %f6;
	cvt.f64.f32 	%fd601, %f7;
	neg.f64 	%fd602, %fd601;
	mov.f64 	%fd603, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd604, %fd602, %fd603, %fd599;
	mov.f64 	%fd605, 0d3E928A27F89B6999;
	mov.f64 	%fd606, 0d3E5AE904A4741B81;
	fma.rn.f64 	%fd607, %fd606, %fd604, %fd605;
	mov.f64 	%fd608, 0d3EC71DE715FF7E07;
	fma.rn.f64 	%fd609, %fd607, %fd604, %fd608;
	mov.f64 	%fd610, 0d3EFA019A6B0AC45A;
	fma.rn.f64 	%fd611, %fd609, %fd604, %fd610;
	mov.f64 	%fd612, 0d3F2A01A017EED94F;
	fma.rn.f64 	%fd613, %fd611, %fd604, %fd612;
	mov.f64 	%fd614, 0d3F56C16C17F2A71B;
	fma.rn.f64 	%fd615, %fd613, %fd604, %fd614;
	mov.f64 	%fd616, 0d3F811111111173C4;
	fma.rn.f64 	%fd617, %fd615, %fd604, %fd616;
	mov.f64 	%fd618, 0d3FA555555555211A;
	fma.rn.f64 	%fd619, %fd617, %fd604, %fd618;
	mov.f64 	%fd620, 0d3FC5555555555540;
	fma.rn.f64 	%fd621, %fd619, %fd604, %fd620;
	mov.f64 	%fd622, 0d3FE0000000000005;
	fma.rn.f64 	%fd623, %fd621, %fd604, %fd622;
	mul.f64 	%fd624, %fd604, %fd623;
	fma.rn.f64 	%fd625, %fd624, %fd604, %fd600;
	add.f64 	%fd626, %fd604, %fd625;
	ex2.approx.ftz.f32 	%f8, %f7;
	cvt.f64.f32 	%fd627, %f8;
	mov.f64 	%fd628, 0d3FF0000000000000;
	sub.f64 	%fd629, %fd628, %fd627;
	neg.f64 	%fd630, %fd626;
	fma.rn.f64 	%fd631, %fd630, %fd627, %fd629;
	setp.ge.f64 	%p51, %fd548, 0d4017AFB48DC96626;
	selp.f64 	%fd632, 0d3FF0000000000000, %fd631, %p51;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r121, %temp}, %fd632;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r122}, %fd632;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r123}, %fd299;
	}
	and.b32  	%r124, %r123, -2147483648;
	or.b32  	%r125, %r122, %r124;
	mov.b64 	%fd633, {%r121, %r125};
	sub.f64 	%fd1534, %fd628, %fd633;
	bra.uni 	$L__BB0_52;

$L__BB0_47:
	setp.gt.f64 	%p47, %fd299, 0d403B4CCCCCCCCCCD;
	mov.f64 	%fd1534, 0d0000000000000000;
	@%p47 bra 	$L__BB0_52;

	setp.lt.s32 	%p48, %r98, 1075052544;
	@%p48 bra 	$L__BB0_50;
	bra.uni 	$L__BB0_49;

$L__BB0_50:
	mov.f64 	%fd421, 0d3FE20DD7452FBC22;
	mov.f64 	%fd423, 0d401FD453E105E9A2;
	// begin inline asm
	fma.rn.f64 	%fd420, %fd421, %fd299, %fd423;
	// end inline asm
	mov.f64 	%fd427, 0d404B26245B951FB4;
	// begin inline asm
	fma.rn.f64 	%fd424, %fd420, %fd299, %fd427;
	// end inline asm
	mov.f64 	%fd431, 0d406C7835DC0F1F49;
	// begin inline asm
	fma.rn.f64 	%fd428, %fd424, %fd299, %fd431;
	// end inline asm
	mov.f64 	%fd435, 0d4083AFA471E5C766;
	// begin inline asm
	fma.rn.f64 	%fd432, %fd428, %fd299, %fd435;
	// end inline asm
	mov.f64 	%fd439, 0d4091FB514824F49F;
	// begin inline asm
	fma.rn.f64 	%fd436, %fd432, %fd299, %fd439;
	// end inline asm
	mov.f64 	%fd443, 0d409450DDEE8272BB;
	// begin inline asm
	fma.rn.f64 	%fd440, %fd436, %fd299, %fd443;
	// end inline asm
	mov.f64 	%fd447, 0d4086B952E4ECBC50;
	// begin inline asm
	fma.rn.f64 	%fd444, %fd440, %fd299, %fd447;
	// end inline asm
	add.f64 	%fd449, %fd299, 0d402C35442E99E667;
	mov.f64 	%fd451, 0d40582F68071A079D;
	// begin inline asm
	fma.rn.f64 	%fd448, %fd449, %fd299, %fd451;
	// end inline asm
	mov.f64 	%fd455, 0d4079ABD39A029DAA;
	// begin inline asm
	fma.rn.f64 	%fd452, %fd448, %fd299, %fd455;
	// end inline asm
	mov.f64 	%fd459, 0d409230CA327093FD;
	// begin inline asm
	fma.rn.f64 	%fd456, %fd452, %fd299, %fd459;
	// end inline asm
	mov.f64 	%fd463, 0d40A174FAB33B54A7;
	// begin inline asm
	fma.rn.f64 	%fd460, %fd456, %fd299, %fd463;
	// end inline asm
	mov.f64 	%fd467, 0d40A601508230F980;
	// begin inline asm
	fma.rn.f64 	%fd464, %fd460, %fd299, %fd467;
	// end inline asm
	mov.f64 	%fd471, 0d40A091785EC9331E;
	// begin inline asm
	fma.rn.f64 	%fd468, %fd464, %fd299, %fd471;
	// end inline asm
	mov.f64 	%fd475, 0d4086B952E52F3622;
	// begin inline asm
	fma.rn.f64 	%fd472, %fd468, %fd299, %fd475;
	// end inline asm
	div.rn.f64 	%fd541, %fd444, %fd472;
	mul.rn.f64 	%fd542, %fd299, %fd299;
	neg.f64 	%fd483, %fd542;
	// begin inline asm
	fma.rn.f64 	%fd476, %fd299, %fd299, %fd483;
	// end inline asm
	mov.f64 	%fd543, 0d3FF71547652B82FE;
	mul.rn.f64 	%fd544, %fd483, %fd543;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r112}, %fd544;
	}
	and.b32  	%r113, %r112, -2147483648;
	mov.f64 	%fd527, 0d3FE0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r114}, %fd527;
	}
	or.b32  	%r115, %r114, %r113;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r116, %temp}, %fd527;
	}
	mov.b64 	%fd545, {%r116, %r115};
	add.rz.f64 	%fd546, %fd544, %fd545;
	cvt.rzi.f64.f64 	%fd485, %fd546;
	cvt.rzi.s32.f64 	%r117, %fd485;
	mov.f64 	%fd482, 0dBFE62E42FEFA39EF;
	// begin inline asm
	fma.rn.f64 	%fd480, %fd485, %fd482, %fd483;
	// end inline asm
	mov.f64 	%fd486, 0dBC7ABC9E3B39803F;
	// begin inline asm
	fma.rn.f64 	%fd484, %fd485, %fd486, %fd480;
	// end inline asm
	setp.lt.s32 	%p50, %r117, -1020;
	selp.f64 	%fd547, 0d3C90000000000000, 0d4000000000000000, %p50;
	mov.f64 	%fd489, 0d3E21F07FCCF58BAD;
	mov.f64 	%fd491, 0d3E5AFD81DA6C3BAF;
	// begin inline asm
	fma.rn.f64 	%fd488, %fd489, %fd484, %fd491;
	// end inline asm
	mov.f64 	%fd495, 0d3E927E55F60F80E6;
	// begin inline asm
	fma.rn.f64 	%fd492, %fd488, %fd484, %fd495;
	// end inline asm
	mov.f64 	%fd499, 0d3EC71DDA8F02D666;
	// begin inline asm
	fma.rn.f64 	%fd496, %fd492, %fd484, %fd499;
	// end inline asm
	mov.f64 	%fd503, 0d3EFA01A013B894E0;
	// begin inline asm
	fma.rn.f64 	%fd500, %fd496, %fd484, %fd503;
	// end inline asm
	mov.f64 	%fd507, 0d3F2A01A01D3AF788;
	// begin inline asm
	fma.rn.f64 	%fd504, %fd500, %fd484, %fd507;
	// end inline asm
	mov.f64 	%fd511, 0d3F56C16C16C3A1EC;
	// begin inline asm
	fma.rn.f64 	%fd508, %fd504, %fd484, %fd511;
	// end inline asm
	mov.f64 	%fd515, 0d3F81111111109161;
	// begin inline asm
	fma.rn.f64 	%fd512, %fd508, %fd484, %fd515;
	// end inline asm
	mov.f64 	%fd519, 0d3FA55555555554C1;
	// begin inline asm
	fma.rn.f64 	%fd516, %fd512, %fd484, %fd519;
	// end inline asm
	mov.f64 	%fd523, 0d3FC555555555556F;
	// begin inline asm
	fma.rn.f64 	%fd520, %fd516, %fd484, %fd523;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd524, %fd520, %fd484, %fd527;
	// end inline asm
	mul.rn.f64 	%fd529, %fd524, %fd484;
	// begin inline asm
	fma.rn.f64 	%fd528, %fd529, %fd484, %fd484;
	// end inline asm
	shl.b32 	%r118, %r117, 20;
	add.s32 	%r119, %r118, 57671680;
	selp.b32 	%r120, %r119, %r118, %p50;
	add.s32 	%r111, %r120, 1071644672;
	mov.u32 	%r110, 0;
	// begin inline asm
	mov.b64 	%fd532, {%r110, %r111};
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd533, %fd528, %fd532, %fd532;
	// end inline asm
	mul.rn.f64 	%fd540, %fd533, %fd547;
	neg.f64 	%fd539, %fd540;
	// begin inline asm
	fma.rn.f64 	%fd537, %fd476, %fd539, %fd540;
	// end inline asm
	mul.rn.f64 	%fd1534, %fd541, %fd537;
	bra.uni 	$L__BB0_52;

$L__BB0_49:
	rcp.rn.f64 	%fd412, %fd299;
	mul.rn.f64 	%fd345, %fd412, %fd412;
	mov.f64 	%fd304, 0dC1186DF84479631D;
	mov.f64 	%fd306, 0d41019A6E9A7FFBB8;
	// begin inline asm
	fma.rn.f64 	%fd303, %fd304, %fd345, %fd306;
	// end inline asm
	mov.f64 	%fd310, 0dC0DB040BE3D5CA18;
	// begin inline asm
	fma.rn.f64 	%fd307, %fd303, %fd345, %fd310;
	// end inline asm
	mov.f64 	%fd314, 0d40B012760EE009A0;
	// begin inline asm
	fma.rn.f64 	%fd311, %fd307, %fd345, %fd314;
	// end inline asm
	mov.f64 	%fd318, 0dC082587AE4008D0E;
	// begin inline asm
	fma.rn.f64 	%fd315, %fd311, %fd345, %fd318;
	// end inline asm
	mov.f64 	%fd322, 0d4056DF5D938ACAFE;
	// begin inline asm
	fma.rn.f64 	%fd319, %fd315, %fd345, %fd322;
	// end inline asm
	mov.f64 	%fd326, 0dC030A8D46D765681;
	// begin inline asm
	fma.rn.f64 	%fd323, %fd319, %fd345, %fd326;
	// end inline asm
	mov.f64 	%fd330, 0d400D9EAE0C665C75;
	// begin inline asm
	fma.rn.f64 	%fd327, %fd323, %fd345, %fd330;
	// end inline asm
	mov.f64 	%fd334, 0dBFF0ECF9C8880942;
	// begin inline asm
	fma.rn.f64 	%fd331, %fd327, %fd345, %fd334;
	// end inline asm
	mov.f64 	%fd338, 0d3FDB14C2F82A33F7;
	// begin inline asm
	fma.rn.f64 	%fd335, %fd331, %fd345, %fd338;
	// end inline asm
	mov.f64 	%fd342, 0dBFD20DD75042844F;
	// begin inline asm
	fma.rn.f64 	%fd339, %fd335, %fd345, %fd342;
	// end inline asm
	mov.f64 	%fd346, 0d3FE20DD750429B6B;
	// begin inline asm
	fma.rn.f64 	%fd343, %fd339, %fd345, %fd346;
	// end inline asm
	mul.rn.f64 	%fd413, %fd299, %fd299;
	neg.f64 	%fd354, %fd413;
	// begin inline asm
	fma.rn.f64 	%fd347, %fd299, %fd299, %fd354;
	// end inline asm
	mov.f64 	%fd414, 0d3FF71547652B82FE;
	mul.rn.f64 	%fd415, %fd354, %fd414;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r101}, %fd415;
	}
	and.b32  	%r102, %r101, -2147483648;
	mov.f64 	%fd398, 0d3FE0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r103}, %fd398;
	}
	or.b32  	%r104, %r103, %r102;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r105, %temp}, %fd398;
	}
	mov.b64 	%fd416, {%r105, %r104};
	add.rz.f64 	%fd417, %fd415, %fd416;
	cvt.rzi.f64.f64 	%fd356, %fd417;
	cvt.rzi.s32.f64 	%r106, %fd356;
	mov.f64 	%fd353, 0dBFE62E42FEFA39EF;
	// begin inline asm
	fma.rn.f64 	%fd351, %fd356, %fd353, %fd354;
	// end inline asm
	mov.f64 	%fd357, 0dBC7ABC9E3B39803F;
	// begin inline asm
	fma.rn.f64 	%fd355, %fd356, %fd357, %fd351;
	// end inline asm
	setp.lt.s32 	%p49, %r106, -1020;
	selp.f64 	%fd418, 0d3C90000000000000, 0d4000000000000000, %p49;
	mov.f64 	%fd360, 0d3E21F07FCCF58BAD;
	mov.f64 	%fd362, 0d3E5AFD81DA6C3BAF;
	// begin inline asm
	fma.rn.f64 	%fd359, %fd360, %fd355, %fd362;
	// end inline asm
	mov.f64 	%fd366, 0d3E927E55F60F80E6;
	// begin inline asm
	fma.rn.f64 	%fd363, %fd359, %fd355, %fd366;
	// end inline asm
	mov.f64 	%fd370, 0d3EC71DDA8F02D666;
	// begin inline asm
	fma.rn.f64 	%fd367, %fd363, %fd355, %fd370;
	// end inline asm
	mov.f64 	%fd374, 0d3EFA01A013B894E0;
	// begin inline asm
	fma.rn.f64 	%fd371, %fd367, %fd355, %fd374;
	// end inline asm
	mov.f64 	%fd378, 0d3F2A01A01D3AF788;
	// begin inline asm
	fma.rn.f64 	%fd375, %fd371, %fd355, %fd378;
	// end inline asm
	mov.f64 	%fd382, 0d3F56C16C16C3A1EC;
	// begin inline asm
	fma.rn.f64 	%fd379, %fd375, %fd355, %fd382;
	// end inline asm
	mov.f64 	%fd386, 0d3F81111111109161;
	// begin inline asm
	fma.rn.f64 	%fd383, %fd379, %fd355, %fd386;
	// end inline asm
	mov.f64 	%fd390, 0d3FA55555555554C1;
	// begin inline asm
	fma.rn.f64 	%fd387, %fd383, %fd355, %fd390;
	// end inline asm
	mov.f64 	%fd394, 0d3FC555555555556F;
	// begin inline asm
	fma.rn.f64 	%fd391, %fd387, %fd355, %fd394;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd395, %fd391, %fd355, %fd398;
	// end inline asm
	mul.rn.f64 	%fd400, %fd395, %fd355;
	// begin inline asm
	fma.rn.f64 	%fd399, %fd400, %fd355, %fd355;
	// end inline asm
	shl.b32 	%r107, %r106, 20;
	add.s32 	%r108, %r107, 57671680;
	selp.b32 	%r109, %r108, %r107, %p49;
	add.s32 	%r100, %r109, 1071644672;
	mov.u32 	%r99, 0;
	// begin inline asm
	mov.b64 	%fd403, {%r99, %r100};
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd404, %fd399, %fd403, %fd403;
	// end inline asm
	mul.rn.f64 	%fd411, %fd404, %fd418;
	neg.f64 	%fd410, %fd411;
	// begin inline asm
	fma.rn.f64 	%fd408, %fd347, %fd410, %fd411;
	// end inline asm
	mul.rn.f64 	%fd419, %fd343, %fd412;
	mul.rn.f64 	%fd1534, %fd419, %fd408;

$L__BB0_52:
	fma.rn.f64 	%fd94, %fd1534, 0d3FE0000000000000, 0d0000000000000000;
	mov.f64 	%fd1535, 0d0000000000000000;
	shl.b64 	%rd46, %rd1, 3;
	add.s64 	%rd2, %rd17, %rd46;
	add.s64 	%rd3, %rd18, %rd46;
	mov.f64 	%fd1536, %fd1535;
	@%p44 bra 	$L__BB0_54;

	ld.global.f64 	%fd636, [%rd2];
	abs.f64 	%fd637, %fd636;
	setp.gtu.f64 	%p53, %fd637, 0d7FF0000000000000;
	add.f64 	%fd638, %fd636, 0d0000000000000000;
	selp.f64 	%fd639, 0d0000000000000000, %fd638, %p53;
	selp.f64 	%fd640, 0d0000000000000000, 0d3FF0000000000000, %p53;
	ld.global.f64 	%fd641, [%rd3];
	abs.f64 	%fd642, %fd641;
	setp.gtu.f64 	%p54, %fd642, 0d7FF0000000000000;
	add.f64 	%fd643, %fd641, %fd639;
	add.f64 	%fd644, %fd640, 0d3FF0000000000000;
	selp.f64 	%fd1535, %fd639, %fd643, %p54;
	selp.f64 	%fd1536, %fd640, %fd644, %p54;

$L__BB0_54:
	setp.eq.f64 	%p55, %fd1536, 0d0000000000000000;
	mov.f64 	%fd1561, 0d7FF8000000000214;
	@%p55 bra 	$L__BB0_210;

	div.rn.f64 	%fd99, %fd1535, %fd1536;
	mov.f64 	%fd1560, 0d0000000000000000;
	@%p44 bra 	$L__BB0_208;

	ld.global.f64 	%fd100, [%rd2];
	abs.f64 	%fd101, %fd100;
	setp.gtu.f64 	%p57, %fd101, 0d7FF0000000000000;
	mov.f64 	%fd1560, 0d0000000000000000;
	@%p57 bra 	$L__BB0_132;

	setp.lt.f64 	%p58, %fd100, 0d0000000000000000;
	setp.lt.f64 	%p59, %fd99, 0d0000000000000000;
	and.pred  	%p60, %p59, %p58;
	@%p60 bra 	$L__BB0_59;

	setp.leu.f64 	%p61, %fd100, 0d0000000000000000;
	setp.leu.f64 	%p62, %fd99, 0d0000000000000000;
	or.pred  	%p63, %p62, %p61;
	@%p63 bra 	$L__BB0_70;

$L__BB0_59:
	setp.eq.f64 	%p64, %fd100, %fd99;
	mov.f64 	%fd105, 0d0000000000000000;
	@%p64 bra 	$L__BB0_71;

	setp.eq.f64 	%p65, %fd100, 0d0000000000000000;
	setp.eq.f64 	%p66, %fd99, 0d0000000000000000;
	or.pred  	%p67, %p66, %p65;
	@%p67 bra 	$L__BB0_70;

	sub.f64 	%fd649, %fd100, %fd99;
	abs.f64 	%fd102, %fd649;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r126}, %fd102;
	}
	and.b32  	%r127, %r126, 2146435072;
	setp.eq.s32 	%p68, %r127, 2146435072;
	mul.f64 	%fd650, %fd101, 0d3D30000000000000;
	setp.gt.f64 	%p69, %fd102, %fd650;
	or.pred  	%p70, %p68, %p69;
	@%p70 bra 	$L__BB0_70;

	abs.f64 	%fd103, %fd99;
	mul.f64 	%fd651, %fd103, 0d3D30000000000000;
	setp.gt.f64 	%p71, %fd102, %fd651;
	@%p71 bra 	$L__BB0_70;

	setp.gtu.f64 	%p72, %fd102, 0d433FFFFFFFFFFFFF;
	@%p72 bra 	$L__BB0_69;

	cvt.rzi.s64.f64 	%rd4, %fd102;
	setp.gt.s64 	%p73, %rd4, 9007199254740991;
	@%p73 bra 	$L__BB0_69;

	cvt.rn.f64.s64 	%fd652, %rd4;
	setp.ne.f64 	%p74, %fd102, %fd652;
	setp.gtu.f64 	%p75, %fd101, 0d433FFFFFFFFFFFFF;
	or.pred  	%p76, %p74, %p75;
	@%p76 bra 	$L__BB0_69;

	cvt.rzi.s64.f64 	%rd5, %fd101;
	setp.gt.s64 	%p77, %rd5, 9007199254740991;
	@%p77 bra 	$L__BB0_69;

	cvt.rn.f64.s64 	%fd653, %rd5;
	setp.ne.f64 	%p78, %fd101, %fd653;
	setp.gtu.f64 	%p79, %fd103, 0d433FFFFFFFFFFFFF;
	or.pred  	%p80, %p78, %p79;
	@%p80 bra 	$L__BB0_69;

	cvt.rzi.s64.f64 	%rd47, %fd103;
	setp.lt.s64 	%p81, %rd47, 9007199254740992;
	cvt.rn.f64.s64 	%fd654, %rd47;
	setp.equ.f64 	%p82, %fd103, %fd654;
	and.pred  	%p83, %p81, %p82;
	@%p83 bra 	$L__BB0_70;

$L__BB0_69:
	mul.f64 	%fd656, %fd101, 0d3CF0000000000000;
	setp.lt.f64 	%p84, %fd102, %fd656;
	mul.f64 	%fd657, %fd103, 0d3CF0000000000000;
	setp.lt.f64 	%p85, %fd102, %fd657;
	and.pred  	%p86, %p84, %p85;
	@%p86 bra 	$L__BB0_71;

$L__BB0_70:
	sub.f64 	%fd105, %fd100, %fd99;

$L__BB0_71:
	abs.f64 	%fd106, %fd105;
	setp.gtu.f64 	%p87, %fd106, 0d7FF0000000000000;
	mov.f64 	%fd1547, 0dFFF8000000000000;
	@%p87 bra 	$L__BB0_131;

	setp.eq.f64 	%p88, %fd105, 0d0000000000000000;
	mov.f64 	%fd1547, 0d0000000000000000;
	mov.f64 	%fd1541, 0d3FF0000000000000;
	@%p88 bra 	$L__BB0_131;

	setp.eq.f64 	%p89, %fd106, 0d3FF0000000000000;
	@%p89 bra 	$L__BB0_99;

	abs.f64 	%fd107, %fd106;
	setp.gtu.f64 	%p90, %fd107, 0d7FF0000000000000;
	@%p90 bra 	$L__BB0_98;
	bra.uni 	$L__BB0_75;

$L__BB0_98:
	add.f64 	%fd1541, %fd106, 0d4000000000000000;
	bra.uni 	$L__BB0_99;

$L__BB0_75:
	setp.eq.f64 	%p91, %fd106, 0d7FF0000000000000;
	@%p91 bra 	$L__BB0_97;
	bra.uni 	$L__BB0_76;

$L__BB0_97:
	mov.f64 	%fd846, 0d4000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r158}, %fd846;
	}
	setp.gt.s32 	%p114, %r158, -1;
	selp.f64 	%fd1541, 0d7FF0000000000000, 0d0000000000000000, %p114;

$L__BB0_99:
	abs.f64 	%fd1461, %fd105;
	setp.eq.f64 	%p352, %fd1461, 0d3FF0000000000000;
	mov.f64 	%fd848, 0d40F0000000000000;
	mov.f64 	%fd849, 0d0000000000000000;
	mul.rn.f64 	%fd126, %fd849, %fd848;
	setp.eq.f64 	%p115, %fd126, 0d0000000000000000;
	mov.f64 	%fd1546, 0d3FF0000000000000;
	or.pred  	%p117, %p352, %p115;
	@%p117 bra 	$L__BB0_130;

	abs.f64 	%fd1477, %fd105;
	abs.f64 	%fd127, %fd1477;
	setp.gtu.f64 	%p118, %fd127, 0d7FF0000000000000;
	@%p118 bra 	$L__BB0_129;

	abs.f64 	%fd128, %fd126;
	setp.gtu.f64 	%p119, %fd128, 0d7FF0000000000000;
	@%p119 bra 	$L__BB0_129;
	bra.uni 	$L__BB0_102;

$L__BB0_129:
	abs.f64 	%fd1484, %fd105;
	add.f64 	%fd1546, %fd1484, %fd126;

$L__BB0_130:
	mul.rn.f64 	%fd1547, %fd1541, %fd1546;

$L__BB0_131:
	add.f64 	%fd1560, %fd1547, 0d0000000000000000;

$L__BB0_132:
	ld.global.f64 	%fd159, [%rd3];
	abs.f64 	%fd160, %fd159;
	setp.gtu.f64 	%p148, %fd160, 0d7FF0000000000000;
	@%p148 bra 	$L__BB0_208;

	setp.lt.f64 	%p149, %fd159, 0d0000000000000000;
	setp.lt.f64 	%p150, %fd99, 0d0000000000000000;
	and.pred  	%p151, %p150, %p149;
	@%p151 bra 	$L__BB0_135;

	setp.leu.f64 	%p152, %fd159, 0d0000000000000000;
	setp.leu.f64 	%p153, %fd99, 0d0000000000000000;
	or.pred  	%p154, %p153, %p152;
	@%p154 bra 	$L__BB0_146;

$L__BB0_135:
	setp.eq.f64 	%p155, %fd159, %fd99;
	mov.f64 	%fd164, 0d0000000000000000;
	@%p155 bra 	$L__BB0_147;

	setp.eq.f64 	%p156, %fd159, 0d0000000000000000;
	setp.eq.f64 	%p157, %fd99, 0d0000000000000000;
	or.pred  	%p158, %p157, %p156;
	@%p158 bra 	$L__BB0_146;

	sub.f64 	%fd1032, %fd159, %fd99;
	abs.f64 	%fd161, %fd1032;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r190}, %fd161;
	}
	and.b32  	%r191, %r190, 2146435072;
	setp.eq.s32 	%p159, %r191, 2146435072;
	mul.f64 	%fd1033, %fd160, 0d3D30000000000000;
	setp.gt.f64 	%p160, %fd161, %fd1033;
	or.pred  	%p161, %p159, %p160;
	@%p161 bra 	$L__BB0_146;

	abs.f64 	%fd162, %fd99;
	mul.f64 	%fd1034, %fd162, 0d3D30000000000000;
	setp.gt.f64 	%p162, %fd161, %fd1034;
	@%p162 bra 	$L__BB0_146;

	setp.gtu.f64 	%p163, %fd161, 0d433FFFFFFFFFFFFF;
	@%p163 bra 	$L__BB0_145;

	cvt.rzi.s64.f64 	%rd6, %fd161;
	setp.gt.s64 	%p164, %rd6, 9007199254740991;
	@%p164 bra 	$L__BB0_145;

	cvt.rn.f64.s64 	%fd1035, %rd6;
	setp.ne.f64 	%p165, %fd161, %fd1035;
	setp.gtu.f64 	%p166, %fd160, 0d433FFFFFFFFFFFFF;
	or.pred  	%p167, %p165, %p166;
	@%p167 bra 	$L__BB0_145;

	cvt.rzi.s64.f64 	%rd7, %fd160;
	setp.gt.s64 	%p168, %rd7, 9007199254740991;
	@%p168 bra 	$L__BB0_145;

	cvt.rn.f64.s64 	%fd1036, %rd7;
	setp.ne.f64 	%p169, %fd160, %fd1036;
	setp.gtu.f64 	%p170, %fd162, 0d433FFFFFFFFFFFFF;
	or.pred  	%p171, %p169, %p170;
	@%p171 bra 	$L__BB0_145;

	cvt.rzi.s64.f64 	%rd54, %fd162;
	setp.lt.s64 	%p172, %rd54, 9007199254740992;
	cvt.rn.f64.s64 	%fd1037, %rd54;
	setp.equ.f64 	%p173, %fd162, %fd1037;
	and.pred  	%p174, %p172, %p173;
	@%p174 bra 	$L__BB0_146;

$L__BB0_145:
	mul.f64 	%fd1039, %fd160, 0d3CF0000000000000;
	setp.lt.f64 	%p175, %fd161, %fd1039;
	mul.f64 	%fd1040, %fd162, 0d3CF0000000000000;
	setp.lt.f64 	%p176, %fd161, %fd1040;
	and.pred  	%p177, %p175, %p176;
	@%p177 bra 	$L__BB0_147;

$L__BB0_146:
	sub.f64 	%fd164, %fd159, %fd99;

$L__BB0_147:
	abs.f64 	%fd165, %fd164;
	setp.gtu.f64 	%p178, %fd165, 0d7FF0000000000000;
	mov.f64 	%fd1559, 0dFFF8000000000000;
	@%p178 bra 	$L__BB0_207;

	setp.eq.f64 	%p179, %fd164, 0d0000000000000000;
	mov.f64 	%fd1559, 0d0000000000000000;
	mov.f64 	%fd1553, 0d3FF0000000000000;
	@%p179 bra 	$L__BB0_207;

	setp.eq.f64 	%p180, %fd165, 0d3FF0000000000000;
	@%p180 bra 	$L__BB0_175;

	abs.f64 	%fd166, %fd165;
	setp.gtu.f64 	%p181, %fd166, 0d7FF0000000000000;
	@%p181 bra 	$L__BB0_174;
	bra.uni 	$L__BB0_151;

$L__BB0_174:
	add.f64 	%fd1553, %fd165, 0d4000000000000000;
	bra.uni 	$L__BB0_175;

$L__BB0_151:
	setp.eq.f64 	%p182, %fd165, 0d7FF0000000000000;
	@%p182 bra 	$L__BB0_173;
	bra.uni 	$L__BB0_152;

$L__BB0_173:
	mov.f64 	%fd1229, 0d4000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r222}, %fd1229;
	}
	setp.gt.s32 	%p205, %r222, -1;
	selp.f64 	%fd1553, 0d7FF0000000000000, 0d0000000000000000, %p205;

$L__BB0_175:
	abs.f64 	%fd1467, %fd164;
	setp.eq.f64 	%p354, %fd1467, 0d3FF0000000000000;
	mov.f64 	%fd1231, 0d40F0000000000000;
	mov.f64 	%fd1232, 0d0000000000000000;
	mul.rn.f64 	%fd185, %fd1232, %fd1231;
	setp.eq.f64 	%p206, %fd185, 0d0000000000000000;
	mov.f64 	%fd1558, 0d3FF0000000000000;
	or.pred  	%p208, %p354, %p206;
	@%p208 bra 	$L__BB0_206;

	abs.f64 	%fd1468, %fd164;
	abs.f64 	%fd186, %fd1468;
	setp.gtu.f64 	%p209, %fd186, 0d7FF0000000000000;
	@%p209 bra 	$L__BB0_205;

	abs.f64 	%fd187, %fd185;
	setp.gtu.f64 	%p210, %fd187, 0d7FF0000000000000;
	@%p210 bra 	$L__BB0_205;
	bra.uni 	$L__BB0_178;

$L__BB0_205:
	abs.f64 	%fd1475, %fd164;
	add.f64 	%fd1558, %fd1475, %fd185;

$L__BB0_206:
	mul.rn.f64 	%fd1559, %fd1553, %fd1558;

$L__BB0_207:
	add.f64 	%fd1560, %fd1560, %fd1559;

$L__BB0_208:
	mov.f64 	%fd1561, 0d7FF8000000000214;
	setp.le.f64 	%p239, %fd1536, 0d3FF0000000000000;
	@%p239 bra 	$L__BB0_210;

	add.f64 	%fd1415, %fd1536, 0dBFF0000000000000;
	div.rn.f64 	%fd1561, %fd1560, %fd1415;

$L__BB0_210:
	setp.gt.f64 	%p240, %fd94, 0d0000000000000000;
	setp.lt.f64 	%p241, %fd1561, 0d0000000000000000;
	and.pred  	%p242, %p240, %p241;
	@%p242 bra 	$L__BB0_212;

	setp.geu.f64 	%p243, %fd94, 0d0000000000000000;
	setp.leu.f64 	%p244, %fd1561, 0d0000000000000000;
	or.pred  	%p245, %p243, %p244;
	@%p245 bra 	$L__BB0_224;

$L__BB0_212:
	neg.f64 	%fd220, %fd94;
	setp.eq.f64 	%p246, %fd1561, %fd220;
	mov.f64 	%fd1562, 0d0000000000000000;
	@%p246 bra 	$L__BB0_225;

	setp.eq.f64 	%p247, %fd1561, 0d0000000000000000;
	setp.eq.f64 	%p248, %fd94, 0d8000000000000000;
	or.pred  	%p249, %p248, %p247;
	@%p249 bra 	$L__BB0_224;

	add.f64 	%fd1417, %fd94, %fd1561;
	abs.f64 	%fd221, %fd1417;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r254}, %fd221;
	}
	and.b32  	%r255, %r254, 2146435072;
	setp.eq.s32 	%p250, %r255, 2146435072;
	@%p250 bra 	$L__BB0_224;

	abs.f64 	%fd222, %fd1561;
	mul.f64 	%fd1418, %fd222, 0d3D30000000000000;
	setp.gt.f64 	%p251, %fd221, %fd1418;
	@%p251 bra 	$L__BB0_224;

	abs.f64 	%fd223, %fd220;
	mul.f64 	%fd1419, %fd223, 0d3D30000000000000;
	setp.gt.f64 	%p252, %fd221, %fd1419;
	@%p252 bra 	$L__BB0_224;

	setp.gtu.f64 	%p253, %fd221, 0d433FFFFFFFFFFFFF;
	@%p253 bra 	$L__BB0_223;

	cvt.rzi.s64.f64 	%rd8, %fd221;
	setp.gt.s64 	%p254, %rd8, 9007199254740991;
	@%p254 bra 	$L__BB0_223;

	cvt.rn.f64.s64 	%fd1420, %rd8;
	setp.ne.f64 	%p255, %fd221, %fd1420;
	setp.gtu.f64 	%p256, %fd222, 0d433FFFFFFFFFFFFF;
	or.pred  	%p257, %p255, %p256;
	@%p257 bra 	$L__BB0_223;

	cvt.rzi.s64.f64 	%rd9, %fd222;
	setp.gt.s64 	%p258, %rd9, 9007199254740991;
	@%p258 bra 	$L__BB0_223;

	cvt.rn.f64.s64 	%fd1421, %rd9;
	setp.ne.f64 	%p259, %fd222, %fd1421;
	setp.gtu.f64 	%p260, %fd223, 0d433FFFFFFFFFFFFF;
	or.pred  	%p261, %p259, %p260;
	@%p261 bra 	$L__BB0_223;

	cvt.rzi.s64.f64 	%rd61, %fd223;
	setp.lt.s64 	%p262, %rd61, 9007199254740992;
	cvt.rn.f64.s64 	%fd1422, %rd61;
	setp.equ.f64 	%p263, %fd223, %fd1422;
	and.pred  	%p264, %p262, %p263;
	@%p264 bra 	$L__BB0_224;

$L__BB0_223:
	mul.f64 	%fd1424, %fd222, 0d3CF0000000000000;
	setp.lt.f64 	%p265, %fd221, %fd1424;
	mul.f64 	%fd1425, %fd223, 0d3CF0000000000000;
	setp.lt.f64 	%p266, %fd221, %fd1425;
	and.pred  	%p267, %p265, %p266;
	@%p267 bra 	$L__BB0_225;

$L__BB0_224:
	add.f64 	%fd1562, %fd94, %fd1561;

$L__BB0_225:
	setp.lt.f64 	%p268, %fd1532, 0d0000000000000000;
	setp.lt.f64 	%p269, %fd1562, 0d0000000000000000;
	and.pred  	%p270, %p268, %p269;
	@%p270 bra 	$L__BB0_227;

	setp.leu.f64 	%p271, %fd1562, 0d0000000000000000;
	setp.leu.f64 	%p272, %fd1532, 0d0000000000000000;
	or.pred  	%p273, %p272, %p271;
	@%p273 bra 	$L__BB0_239;

$L__BB0_227:
	setp.eq.f64 	%p274, %fd1562, %fd1532;
	mov.f64 	%fd1563, 0d0000000000000000;
	@%p274 bra 	$L__BB0_240;

	setp.eq.f64 	%p275, %fd1562, 0d0000000000000000;
	setp.eq.f64 	%p276, %fd1532, 0d0000000000000000;
	or.pred  	%p277, %p276, %p275;
	@%p277 bra 	$L__BB0_239;

	sub.f64 	%fd1427, %fd1562, %fd1532;
	abs.f64 	%fd226, %fd1427;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r256}, %fd226;
	}
	and.b32  	%r257, %r256, 2146435072;
	setp.eq.s32 	%p278, %r257, 2146435072;
	@%p278 bra 	$L__BB0_239;

	abs.f64 	%fd227, %fd1562;
	mul.f64 	%fd1428, %fd227, 0d3D30000000000000;
	setp.gt.f64 	%p279, %fd226, %fd1428;
	@%p279 bra 	$L__BB0_239;

	abs.f64 	%fd228, %fd1532;
	mul.f64 	%fd1429, %fd228, 0d3D30000000000000;
	setp.gt.f64 	%p280, %fd226, %fd1429;
	@%p280 bra 	$L__BB0_239;

	setp.gtu.f64 	%p281, %fd226, 0d433FFFFFFFFFFFFF;
	@%p281 bra 	$L__BB0_238;

	cvt.rzi.s64.f64 	%rd10, %fd226;
	setp.gt.s64 	%p282, %rd10, 9007199254740991;
	@%p282 bra 	$L__BB0_238;

	cvt.rn.f64.s64 	%fd1430, %rd10;
	setp.ne.f64 	%p283, %fd226, %fd1430;
	setp.gtu.f64 	%p284, %fd227, 0d433FFFFFFFFFFFFF;
	or.pred  	%p285, %p283, %p284;
	@%p285 bra 	$L__BB0_238;

	cvt.rzi.s64.f64 	%rd11, %fd227;
	setp.gt.s64 	%p286, %rd11, 9007199254740991;
	@%p286 bra 	$L__BB0_238;

	cvt.rn.f64.s64 	%fd1431, %rd11;
	setp.ne.f64 	%p287, %fd227, %fd1431;
	setp.gtu.f64 	%p288, %fd228, 0d433FFFFFFFFFFFFF;
	or.pred  	%p289, %p287, %p288;
	@%p289 bra 	$L__BB0_238;

	cvt.rzi.s64.f64 	%rd62, %fd228;
	setp.lt.s64 	%p290, %rd62, 9007199254740992;
	cvt.rn.f64.s64 	%fd1432, %rd62;
	setp.equ.f64 	%p291, %fd228, %fd1432;
	and.pred  	%p292, %p290, %p291;
	@%p292 bra 	$L__BB0_239;

$L__BB0_238:
	mul.f64 	%fd1434, %fd227, 0d3CF0000000000000;
	setp.lt.f64 	%p293, %fd226, %fd1434;
	mul.f64 	%fd1435, %fd228, 0d3CF0000000000000;
	setp.lt.f64 	%p294, %fd226, %fd1435;
	and.pred  	%p295, %p293, %p294;
	@%p295 bra 	$L__BB0_240;

$L__BB0_239:
	sub.f64 	%fd1563, %fd1562, %fd1532;

$L__BB0_240:
	setp.gt.f64 	%p296, %fd62, 0d0000000000000000;
	setp.lt.f64 	%p297, %fd1563, 0d0000000000000000;
	and.pred  	%p298, %p296, %p297;
	@%p298 bra 	$L__BB0_242;

	setp.geu.f64 	%p299, %fd62, 0d0000000000000000;
	setp.leu.f64 	%p300, %fd1563, 0d0000000000000000;
	or.pred  	%p301, %p299, %p300;
	@%p301 bra 	$L__BB0_254;

$L__BB0_242:
	neg.f64 	%fd231, %fd62;
	setp.eq.f64 	%p302, %fd1563, %fd231;
	mov.f64 	%fd1564, 0d0000000000000000;
	@%p302 bra 	$L__BB0_255;

	setp.eq.f64 	%p303, %fd1563, 0d0000000000000000;
	setp.eq.f64 	%p304, %fd62, 0d8000000000000000;
	or.pred  	%p305, %p304, %p303;
	@%p305 bra 	$L__BB0_254;

	add.f64 	%fd1437, %fd62, %fd1563;
	abs.f64 	%fd232, %fd1437;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r258}, %fd232;
	}
	and.b32  	%r259, %r258, 2146435072;
	setp.eq.s32 	%p306, %r259, 2146435072;
	@%p306 bra 	$L__BB0_254;

	abs.f64 	%fd233, %fd1563;
	mul.f64 	%fd1438, %fd233, 0d3D30000000000000;
	setp.gt.f64 	%p307, %fd232, %fd1438;
	@%p307 bra 	$L__BB0_254;

	abs.f64 	%fd234, %fd231;
	mul.f64 	%fd1439, %fd234, 0d3D30000000000000;
	setp.gt.f64 	%p308, %fd232, %fd1439;
	@%p308 bra 	$L__BB0_254;

	setp.gtu.f64 	%p309, %fd232, 0d433FFFFFFFFFFFFF;
	@%p309 bra 	$L__BB0_253;

	cvt.rzi.s64.f64 	%rd12, %fd232;
	setp.gt.s64 	%p310, %rd12, 9007199254740991;
	@%p310 bra 	$L__BB0_253;

	cvt.rn.f64.s64 	%fd1440, %rd12;
	setp.ne.f64 	%p311, %fd232, %fd1440;
	setp.gtu.f64 	%p312, %fd233, 0d433FFFFFFFFFFFFF;
	or.pred  	%p313, %p311, %p312;
	@%p313 bra 	$L__BB0_253;

	cvt.rzi.s64.f64 	%rd13, %fd233;
	setp.gt.s64 	%p314, %rd13, 9007199254740991;
	@%p314 bra 	$L__BB0_253;

	cvt.rn.f64.s64 	%fd1441, %rd13;
	setp.ne.f64 	%p315, %fd233, %fd1441;
	setp.gtu.f64 	%p316, %fd234, 0d433FFFFFFFFFFFFF;
	or.pred  	%p317, %p315, %p316;
	@%p317 bra 	$L__BB0_253;

	cvt.rzi.s64.f64 	%rd63, %fd234;
	setp.lt.s64 	%p318, %rd63, 9007199254740992;
	cvt.rn.f64.s64 	%fd1442, %rd63;
	setp.equ.f64 	%p319, %fd234, %fd1442;
	and.pred  	%p320, %p318, %p319;
	@%p320 bra 	$L__BB0_254;

$L__BB0_253:
	mul.f64 	%fd1444, %fd233, 0d3CF0000000000000;
	setp.lt.f64 	%p321, %fd232, %fd1444;
	mul.f64 	%fd1445, %fd234, 0d3CF0000000000000;
	setp.lt.f64 	%p322, %fd232, %fd1445;
	and.pred  	%p323, %p321, %p322;
	@%p323 bra 	$L__BB0_255;

$L__BB0_254:
	add.f64 	%fd1564, %fd62, %fd1563;

$L__BB0_255:
	setp.gt.f64 	%p324, %fd29, 0d0000000000000000;
	setp.lt.f64 	%p325, %fd1564, 0d0000000000000000;
	and.pred  	%p326, %p324, %p325;
	@%p326 bra 	$L__BB0_257;

	setp.geu.f64 	%p327, %fd29, 0d0000000000000000;
	setp.leu.f64 	%p328, %fd1564, 0d0000000000000000;
	or.pred  	%p329, %p327, %p328;
	@%p329 bra 	$L__BB0_269;

$L__BB0_257:
	neg.f64 	%fd237, %fd29;
	setp.eq.f64 	%p330, %fd1564, %fd237;
	mov.f64 	%fd1565, 0d0000000000000000;
	@%p330 bra 	$L__BB0_270;

	setp.eq.f64 	%p331, %fd1564, 0d0000000000000000;
	setp.eq.f64 	%p332, %fd29, 0d8000000000000000;
	or.pred  	%p333, %p332, %p331;
	@%p333 bra 	$L__BB0_269;

	add.f64 	%fd1447, %fd29, %fd1564;
	abs.f64 	%fd238, %fd1447;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r260}, %fd238;
	}
	and.b32  	%r261, %r260, 2146435072;
	setp.eq.s32 	%p334, %r261, 2146435072;
	@%p334 bra 	$L__BB0_269;

	abs.f64 	%fd239, %fd1564;
	mul.f64 	%fd1448, %fd239, 0d3D30000000000000;
	setp.gt.f64 	%p335, %fd238, %fd1448;
	@%p335 bra 	$L__BB0_269;

	abs.f64 	%fd240, %fd237;
	mul.f64 	%fd1449, %fd240, 0d3D30000000000000;
	setp.gt.f64 	%p336, %fd238, %fd1449;
	@%p336 bra 	$L__BB0_269;

	setp.gtu.f64 	%p337, %fd238, 0d433FFFFFFFFFFFFF;
	@%p337 bra 	$L__BB0_268;

	cvt.rzi.s64.f64 	%rd14, %fd238;
	setp.gt.s64 	%p338, %rd14, 9007199254740991;
	@%p338 bra 	$L__BB0_268;

	cvt.rn.f64.s64 	%fd1450, %rd14;
	setp.ne.f64 	%p339, %fd238, %fd1450;
	setp.gtu.f64 	%p340, %fd239, 0d433FFFFFFFFFFFFF;
	or.pred  	%p341, %p339, %p340;
	@%p341 bra 	$L__BB0_268;

	cvt.rzi.s64.f64 	%rd15, %fd239;
	setp.gt.s64 	%p342, %rd15, 9007199254740991;
	@%p342 bra 	$L__BB0_268;

	cvt.rn.f64.s64 	%fd1451, %rd15;
	setp.ne.f64 	%p343, %fd239, %fd1451;
	setp.gtu.f64 	%p344, %fd240, 0d433FFFFFFFFFFFFF;
	or.pred  	%p345, %p343, %p344;
	@%p345 bra 	$L__BB0_268;

	cvt.rzi.s64.f64 	%rd64, %fd240;
	setp.lt.s64 	%p346, %rd64, 9007199254740992;
	cvt.rn.f64.s64 	%fd1452, %rd64;
	setp.equ.f64 	%p347, %fd240, %fd1452;
	and.pred  	%p348, %p346, %p347;
	@%p348 bra 	$L__BB0_269;

$L__BB0_268:
	mul.f64 	%fd1454, %fd239, 0d3CF0000000000000;
	setp.lt.f64 	%p349, %fd238, %fd1454;
	mul.f64 	%fd1455, %fd240, 0d3CF0000000000000;
	setp.lt.f64 	%p350, %fd238, %fd1455;
	and.pred  	%p351, %p349, %p350;
	@%p351 bra 	$L__BB0_270;

$L__BB0_269:
	add.f64 	%fd1565, %fd29, %fd1564;

$L__BB0_270:
	mov.b32 	%r268, %envreg3;
	mov.u32 	%r267, %tid.x;
	add.s32 	%r266, %r267, %r268;
	mov.u32 	%r265, %ctaid.x;
	mov.u32 	%r264, %ntid.x;
	mad.lo.s32 	%r263, %r264, %r265, %r266;
	cvt.s64.s32 	%rd69, %r263;
	shl.b64 	%rd68, %rd69, 3;
	ld.param.u64 	%rd67, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_0];
	add.s64 	%rd66, %rd67, %rd68;
	st.global.f64 	[%rd66], %fd1565;
	ret;

$L__BB0_102:
	abs.f64 	%fd1478, %fd105;
	setp.eq.f64 	%p120, %fd1478, 0d7FF0000000000000;
	@%p120 bra 	$L__BB0_128;
	bra.uni 	$L__BB0_103;

$L__BB0_128:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r189}, %fd126;
	}
	setp.gt.s32 	%p147, %r189, -1;
	selp.f64 	%fd1546, 0d7FF0000000000000, 0d0000000000000000, %p147;
	bra.uni 	$L__BB0_130;

$L__BB0_178:
	abs.f64 	%fd1469, %fd164;
	setp.eq.f64 	%p211, %fd1469, 0d7FF0000000000000;
	@%p211 bra 	$L__BB0_204;
	bra.uni 	$L__BB0_179;

$L__BB0_204:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r253}, %fd185;
	}
	setp.gt.s32 	%p238, %r253, -1;
	selp.f64 	%fd1558, 0d7FF0000000000000, 0d0000000000000000, %p238;
	bra.uni 	$L__BB0_206;

$L__BB0_76:
	mov.f64 	%fd661, 0d4000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r128, %temp}, %fd661;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r129}, %fd661;
	}
	and.b32  	%r130, %r129, 2147483647;
	setp.ne.s32 	%p92, %r130, 2146435072;
	setp.ne.s32 	%p93, %r128, 0;
	or.pred  	%p94, %p93, %p92;
	@%p94 bra 	$L__BB0_79;
	bra.uni 	$L__BB0_77;

$L__BB0_79:
	mov.f64 	%fd664, 0d3FE0000000000000;
	mul.rn.f64 	%fd665, %fd664, %fd661;
	cvt.rzi.f64.f64 	%fd666, %fd665;
	mul.rn.f64 	%fd667, %fd661, %fd666;
	sub.f64 	%fd668, %fd661, %fd667;
	abs.f64 	%fd109, %fd668;
	setp.eq.f64 	%p97, %fd106, 0d0000000000000000;
	@%p97 bra 	$L__BB0_96;
	bra.uni 	$L__BB0_80;

$L__BB0_96:
	setp.eq.f64 	%p113, %fd109, 0d3FF0000000000000;
	selp.f64 	%fd1541, %fd106, 0d0000000000000000, %p113;
	bra.uni 	$L__BB0_99;

$L__BB0_152:
	mov.f64 	%fd1044, 0d4000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r192, %temp}, %fd1044;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r193}, %fd1044;
	}
	and.b32  	%r194, %r193, 2147483647;
	setp.ne.s32 	%p183, %r194, 2146435072;
	setp.ne.s32 	%p184, %r192, 0;
	or.pred  	%p185, %p184, %p183;
	@%p185 bra 	$L__BB0_155;
	bra.uni 	$L__BB0_153;

$L__BB0_155:
	mov.f64 	%fd1047, 0d3FE0000000000000;
	mul.rn.f64 	%fd1048, %fd1047, %fd1044;
	cvt.rzi.f64.f64 	%fd1049, %fd1048;
	mul.rn.f64 	%fd1050, %fd1044, %fd1049;
	sub.f64 	%fd1051, %fd1044, %fd1050;
	abs.f64 	%fd168, %fd1051;
	setp.eq.f64 	%p188, %fd165, 0d0000000000000000;
	@%p188 bra 	$L__BB0_172;
	bra.uni 	$L__BB0_156;

$L__BB0_172:
	setp.eq.f64 	%p204, %fd168, 0d3FF0000000000000;
	selp.f64 	%fd1553, %fd165, 0d0000000000000000, %p204;
	bra.uni 	$L__BB0_175;

$L__BB0_103:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r159, %temp}, %fd126;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r160}, %fd126;
	}
	and.b32  	%r161, %r160, 2147483647;
	setp.ne.s32 	%p121, %r161, 2146435072;
	setp.ne.s32 	%p122, %r159, 0;
	or.pred  	%p123, %p122, %p121;
	@%p123 bra 	$L__BB0_107;
	bra.uni 	$L__BB0_104;

$L__BB0_107:
	abs.f64 	%fd1480, %fd105;
	mov.f64 	%fd851, 0d3FE0000000000000;
	mul.rn.f64 	%fd852, %fd851, %fd126;
	cvt.rzi.f64.f64 	%fd853, %fd852;
	mov.f64 	%fd854, 0d4000000000000000;
	mul.rn.f64 	%fd855, %fd854, %fd853;
	sub.f64 	%fd856, %fd126, %fd855;
	abs.f64 	%fd131, %fd856;
	setp.eq.f64 	%p127, %fd1480, 0d0000000000000000;
	@%p127 bra 	$L__BB0_126;
	bra.uni 	$L__BB0_108;

$L__BB0_126:
	abs.f64 	%fd1483, %fd105;
	setp.eq.f64 	%p145, %fd131, 0d3FF0000000000000;
	selp.f64 	%fd1546, %fd1483, 0d0000000000000000, %p145;
	setp.geu.f64 	%p146, %fd126, 0d0000000000000000;
	@%p146 bra 	$L__BB0_130;

	rcp.rn.f64 	%fd1546, %fd1546;
	bra.uni 	$L__BB0_130;

$L__BB0_179:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r223, %temp}, %fd185;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r224}, %fd185;
	}
	and.b32  	%r225, %r224, 2147483647;
	setp.ne.s32 	%p212, %r225, 2146435072;
	setp.ne.s32 	%p213, %r223, 0;
	or.pred  	%p214, %p213, %p212;
	@%p214 bra 	$L__BB0_183;
	bra.uni 	$L__BB0_180;

$L__BB0_183:
	abs.f64 	%fd1471, %fd164;
	mov.f64 	%fd1234, 0d3FE0000000000000;
	mul.rn.f64 	%fd1235, %fd1234, %fd185;
	cvt.rzi.f64.f64 	%fd1236, %fd1235;
	mov.f64 	%fd1237, 0d4000000000000000;
	mul.rn.f64 	%fd1238, %fd1237, %fd1236;
	sub.f64 	%fd1239, %fd185, %fd1238;
	abs.f64 	%fd190, %fd1239;
	setp.eq.f64 	%p218, %fd1471, 0d0000000000000000;
	@%p218 bra 	$L__BB0_202;
	bra.uni 	$L__BB0_184;

$L__BB0_202:
	abs.f64 	%fd1474, %fd164;
	setp.eq.f64 	%p236, %fd190, 0d3FF0000000000000;
	selp.f64 	%fd1558, %fd1474, 0d0000000000000000, %p236;
	setp.geu.f64 	%p237, %fd185, 0d0000000000000000;
	@%p237 bra 	$L__BB0_206;

	rcp.rn.f64 	%fd1558, %fd1558;
	bra.uni 	$L__BB0_206;

$L__BB0_77:
	setp.eq.f64 	%p95, %fd106, 0dBFF0000000000000;
	@%p95 bra 	$L__BB0_99;

	setp.gt.f64 	%p96, %fd107, 0d3FF0000000000000;
	selp.f64 	%fd1541, 0d7FF0000000000000, 0d0000000000000000, %p96;
	bra.uni 	$L__BB0_99;

$L__BB0_153:
	setp.eq.f64 	%p186, %fd165, 0dBFF0000000000000;
	@%p186 bra 	$L__BB0_175;

	setp.gt.f64 	%p187, %fd166, 0d3FF0000000000000;
	selp.f64 	%fd1553, 0d7FF0000000000000, 0d0000000000000000, %p187;
	bra.uni 	$L__BB0_175;

$L__BB0_104:
	abs.f64 	%fd1479, %fd105;
	setp.eq.f64 	%p124, %fd1479, 0dBFF0000000000000;
	@%p124 bra 	$L__BB0_130;

	setp.gt.f64 	%p125, %fd127, 0d3FF0000000000000;
	selp.f64 	%fd1546, 0d7FF0000000000000, 0d0000000000000000, %p125;
	setp.geu.f64 	%p126, %fd126, 0d0000000000000000;
	@%p126 bra 	$L__BB0_130;

	rcp.rn.f64 	%fd1546, %fd1546;
	bra.uni 	$L__BB0_130;

$L__BB0_180:
	abs.f64 	%fd1470, %fd164;
	setp.eq.f64 	%p215, %fd1470, 0dBFF0000000000000;
	@%p215 bra 	$L__BB0_206;

	setp.gt.f64 	%p216, %fd186, 0d3FF0000000000000;
	selp.f64 	%fd1558, 0d7FF0000000000000, 0d0000000000000000, %p216;
	setp.geu.f64 	%p217, %fd185, 0d0000000000000000;
	@%p217 bra 	$L__BB0_206;

	rcp.rn.f64 	%fd1558, %fd1558;
	bra.uni 	$L__BB0_206;

$L__BB0_80:
	setp.eq.f64 	%p98, %fd106, 0dFFF0000000000000;
	@%p98 bra 	$L__BB0_94;
	bra.uni 	$L__BB0_81;

$L__BB0_94:
	setp.neu.f64 	%p112, %fd109, 0d3FF0000000000000;
	mov.f64 	%fd1541, 0d7FF0000000000000;
	@%p112 bra 	$L__BB0_99;

	mov.f64 	%fd1541, 0dFFF0000000000000;
	bra.uni 	$L__BB0_99;

$L__BB0_156:
	setp.eq.f64 	%p189, %fd165, 0dFFF0000000000000;
	@%p189 bra 	$L__BB0_170;
	bra.uni 	$L__BB0_157;

$L__BB0_170:
	setp.neu.f64 	%p203, %fd168, 0d3FF0000000000000;
	mov.f64 	%fd1553, 0d7FF0000000000000;
	@%p203 bra 	$L__BB0_175;

	mov.f64 	%fd1553, 0dFFF0000000000000;
	bra.uni 	$L__BB0_175;

$L__BB0_108:
	abs.f64 	%fd1481, %fd105;
	setp.eq.f64 	%p128, %fd1481, 0dFFF0000000000000;
	@%p128 bra 	$L__BB0_124;
	bra.uni 	$L__BB0_109;

$L__BB0_124:
	setp.neu.f64 	%p143, %fd131, 0d3FF0000000000000;
	setp.lt.f64 	%p144, %fd126, 0d0000000000000000;
	selp.f64 	%fd1546, 0d0000000000000000, 0d7FF0000000000000, %p144;
	@%p143 bra 	$L__BB0_130;

	mov.b64 	%rd52, %fd1546;
	xor.b64  	%rd53, %rd52, -9223372036854775808;
	mov.b64 	%fd1546, %rd53;
	bra.uni 	$L__BB0_130;

$L__BB0_184:
	abs.f64 	%fd1472, %fd164;
	setp.eq.f64 	%p219, %fd1472, 0dFFF0000000000000;
	@%p219 bra 	$L__BB0_200;
	bra.uni 	$L__BB0_185;

$L__BB0_200:
	setp.neu.f64 	%p234, %fd190, 0d3FF0000000000000;
	setp.lt.f64 	%p235, %fd185, 0d0000000000000000;
	selp.f64 	%fd1558, 0d0000000000000000, 0d7FF0000000000000, %p235;
	@%p234 bra 	$L__BB0_206;

	mov.b64 	%rd59, %fd1558;
	xor.b64  	%rd60, %rd59, -9223372036854775808;
	mov.b64 	%fd1558, %rd60;
	bra.uni 	$L__BB0_206;

$L__BB0_81:
	setp.geu.f64 	%p99, %fd106, 0d0000000000000000;
	@%p99 bra 	$L__BB0_83;

	mov.f64 	%fd670, 0d4000000000000000;
	cvt.rzi.f64.f64 	%fd671, %fd670;
	setp.neu.f64 	%p100, %fd671, 0d4000000000000000;
	mov.f64 	%fd1541, 0dFFF8000000000000;
	@%p100 bra 	$L__BB0_99;

$L__BB0_83:
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r283}, %fd107; 
	}
	// end inline asm
	// begin inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r282, hi}, %fd107; 
	}
	// end inline asm
	bfe.u32 	%r284, %r283, 20, 11;
	setp.ne.s32 	%p101, %r284, 0;
	@%p101 bra 	$L__BB0_85;

	mov.f64 	%fd676, 0d4350000000000000;
	mul.rn.f64 	%fd675, %fd107, %fd676;
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r283}, %fd675; 
	}
	// end inline asm
	// begin inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r282, hi}, %fd675; 
	}
	// end inline asm
	bfe.u32 	%r135, %r283, 20, 11;
	add.s32 	%r284, %r135, -54;

$L__BB0_85:
	and.b32  	%r138, %r283, -2146435073;
	or.b32  	%r137, %r138, 1072693248;
	// begin inline asm
	mov.b64 	%fd1538, {%r282, %r137};
	// end inline asm
	setp.lt.u32 	%p102, %r137, 1073127583;
	add.s32 	%r285, %r284, -1023;
	@%p102 bra 	$L__BB0_87;

	// begin inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r139, hi}, %fd1538; 
	}
	// end inline asm
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r140}, %fd1538; 
	}
	// end inline asm
	add.s32 	%r142, %r140, -1048576;
	// begin inline asm
	mov.b64 	%fd1538, {%r139, %r142};
	// end inline asm
	add.s32 	%r285, %r284, -1022;

$L__BB0_87:
	add.f64 	%fd765, %fd1538, 0d3FF0000000000000;
	mov.f64 	%fd766, 0d3FF0000000000000;
	rcp.rn.f64 	%fd767, %fd765;
	add.f64 	%fd707, %fd1538, 0dBFF0000000000000;
	mul.rn.f64 	%fd768, %fd707, %fd767;
	add.f64 	%fd755, %fd768, %fd768;
	mul.rn.f64 	%fd703, %fd755, %fd755;
	mov.f64 	%fd682, 0d3EB0F5FF7D2CAFE2;
	mov.f64 	%fd684, 0d3ED0F5D241AD3B5A;
	// begin inline asm
	fma.rn.f64 	%fd681, %fd682, %fd703, %fd684;
	// end inline asm
	mov.f64 	%fd688, 0d3EF3B20A75488A3F;
	// begin inline asm
	fma.rn.f64 	%fd685, %fd681, %fd703, %fd688;
	// end inline asm
	mov.f64 	%fd692, 0d3F1745CDE4FAECD5;
	// begin inline asm
	fma.rn.f64 	%fd689, %fd685, %fd703, %fd692;
	// end inline asm
	mov.f64 	%fd696, 0d3F3C71C7258A578B;
	// begin inline asm
	fma.rn.f64 	%fd693, %fd689, %fd703, %fd696;
	// end inline asm
	mov.f64 	%fd700, 0d3F6249249242B910;
	// begin inline asm
	fma.rn.f64 	%fd697, %fd693, %fd703, %fd700;
	// end inline asm
	mov.f64 	%fd704, 0d3F89999999999DFB;
	// begin inline asm
	fma.rn.f64 	%fd701, %fd697, %fd703, %fd704;
	// end inline asm
	mul.rn.f64 	%fd769, %fd701, %fd703;
	sub.f64 	%fd770, %fd707, %fd755;
	mov.f64 	%fd763, 0d4000000000000000;
	mul.rn.f64 	%fd708, %fd763, %fd770;
	neg.f64 	%fd706, %fd755;
	// begin inline asm
	fma.rn.f64 	%fd705, %fd706, %fd707, %fd708;
	// end inline asm
	mul.rn.f64 	%fd751, %fd767, %fd705;
	add.f64 	%fd771, %fd769, 0d3FB5555555555555;
	mov.f64 	%fd772, 0d3FB5555555555555;
	sub.f64 	%fd773, %fd772, %fd771;
	add.f64 	%fd774, %fd769, %fd773;
	add.f64 	%fd775, %fd774, 0d0000000000000000;
	add.f64 	%fd776, %fd775, 0dBC46A4CB00B9E7B0;
	add.f64 	%fd718, %fd771, %fd776;
	sub.f64 	%fd777, %fd771, %fd718;
	add.f64 	%fd722, %fd776, %fd777;
	mul.rn.f64 	%fd778, %fd718, %fd755;
	neg.f64 	%fd712, %fd778;
	// begin inline asm
	fma.rn.f64 	%fd709, %fd718, %fd755, %fd712;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd713, %fd722, %fd751, %fd709;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd717, %fd718, %fd751, %fd713;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd721, %fd722, %fd755, %fd717;
	// end inline asm
	add.f64 	%fd734, %fd778, %fd721;
	sub.f64 	%fd779, %fd778, %fd734;
	add.f64 	%fd738, %fd721, %fd779;
	mul.rn.f64 	%fd780, %fd734, %fd755;
	neg.f64 	%fd728, %fd780;
	// begin inline asm
	fma.rn.f64 	%fd725, %fd734, %fd755, %fd728;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd729, %fd738, %fd751, %fd725;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd733, %fd734, %fd751, %fd729;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd737, %fd738, %fd755, %fd733;
	// end inline asm
	add.f64 	%fd750, %fd780, %fd737;
	sub.f64 	%fd781, %fd780, %fd750;
	add.f64 	%fd754, %fd737, %fd781;
	mul.rn.f64 	%fd782, %fd750, %fd755;
	neg.f64 	%fd744, %fd782;
	// begin inline asm
	fma.rn.f64 	%fd741, %fd750, %fd755, %fd744;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd745, %fd754, %fd751, %fd741;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd749, %fd750, %fd751, %fd745;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd753, %fd754, %fd755, %fd749;
	// end inline asm
	add.f64 	%fd783, %fd782, %fd753;
	sub.f64 	%fd784, %fd782, %fd783;
	add.f64 	%fd785, %fd753, %fd784;
	add.f64 	%fd786, %fd755, %fd783;
	sub.f64 	%fd787, %fd755, %fd786;
	add.f64 	%fd788, %fd783, %fd787;
	add.f64 	%fd789, %fd785, %fd788;
	add.f64 	%fd790, %fd751, %fd789;
	add.f64 	%fd791, %fd786, %fd790;
	sub.f64 	%fd792, %fd786, %fd791;
	add.f64 	%fd793, %fd790, %fd792;
	cvt.rn.f64.s32 	%fd794, %r285;
	mov.f64 	%fd795, 0d3FE62E42FEFA3000;
	mul.rn.f64 	%fd796, %fd794, %fd795;
	mov.f64 	%fd797, 0d3D53DE6AF278ECE6;
	mul.rn.f64 	%fd798, %fd794, %fd797;
	add.f64 	%fd799, %fd796, %fd791;
	sub.f64 	%fd800, %fd796, %fd799;
	add.f64 	%fd801, %fd791, %fd800;
	add.f64 	%fd802, %fd793, %fd801;
	add.f64 	%fd803, %fd798, %fd802;
	add.f64 	%fd758, %fd799, %fd803;
	sub.f64 	%fd804, %fd799, %fd758;
	add.f64 	%fd762, %fd803, %fd804;
	mul.rn.f64 	%fd805, %fd758, %fd763;
	neg.f64 	%fd760, %fd805;
	// begin inline asm
	fma.rn.f64 	%fd757, %fd758, %fd763, %fd760;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd761, %fd762, %fd763, %fd757;
	// end inline asm
	add.f64 	%fd113, %fd805, %fd761;
	sub.f64 	%fd806, %fd805, %fd113;
	add.f64 	%fd114, %fd761, %fd806;
	mov.f64 	%fd807, 0d4338000000000000;
	mov.f64 	%fd808, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd809, %fd113, %fd808, %fd807;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r33, %temp}, %fd809;
	}
	mov.f64 	%fd810, 0dC338000000000000;
	add.rn.f64 	%fd811, %fd809, %fd810;
	mov.f64 	%fd812, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd813, %fd811, %fd812, %fd113;
	mov.f64 	%fd814, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd815, %fd811, %fd814, %fd813;
	mov.f64 	%fd816, 0d3E928AF3FCA213EA;
	mov.f64 	%fd817, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd818, %fd817, %fd815, %fd816;
	mov.f64 	%fd819, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd820, %fd818, %fd815, %fd819;
	mov.f64 	%fd821, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd822, %fd820, %fd815, %fd821;
	mov.f64 	%fd823, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd824, %fd822, %fd815, %fd823;
	mov.f64 	%fd825, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd826, %fd824, %fd815, %fd825;
	mov.f64 	%fd827, 0d3F81111111122322;
	fma.rn.f64 	%fd828, %fd826, %fd815, %fd827;
	mov.f64 	%fd829, 0d3FA55555555502A1;
	fma.rn.f64 	%fd830, %fd828, %fd815, %fd829;
	mov.f64 	%fd831, 0d3FC5555555555511;
	fma.rn.f64 	%fd832, %fd830, %fd815, %fd831;
	mov.f64 	%fd833, 0d3FE000000000000B;
	fma.rn.f64 	%fd834, %fd832, %fd815, %fd833;
	fma.rn.f64 	%fd835, %fd834, %fd815, %fd766;
	fma.rn.f64 	%fd836, %fd835, %fd815, %fd766;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r34, %temp}, %fd836;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd836;
	}
	shl.b32 	%r143, %r33, 20;
	add.s32 	%r144, %r35, %r143;
	mov.b64 	%fd1541, {%r34, %r144};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r145}, %fd113;
	}
	mov.b32 	%f9, %r145;
	abs.f32 	%f1, %f9;
	setp.lt.f32 	%p103, %f1, 0f4086232B;
	@%p103 bra 	$L__BB0_90;

	setp.lt.f64 	%p104, %fd113, 0d0000000000000000;
	add.f64 	%fd837, %fd113, 0d7FF0000000000000;
	selp.f64 	%fd1541, 0d0000000000000000, %fd837, %p104;
	setp.geu.f32 	%p105, %f1, 0f40874800;
	@%p105 bra 	$L__BB0_90;

	mov.f64 	%fd1460, 0d4338000000000000;
	mov.f64 	%fd1459, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd1458, %fd113, %fd1459, %fd1460;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r262, %temp}, %fd1458;
	}
	shr.u32 	%r146, %r262, 31;
	add.s32 	%r147, %r262, %r146;
	shr.s32 	%r148, %r147, 1;
	shl.b32 	%r149, %r148, 20;
	add.s32 	%r150, %r35, %r149;
	mov.b64 	%fd838, {%r34, %r150};
	sub.s32 	%r151, %r262, %r148;
	shl.b32 	%r152, %r151, 20;
	add.s32 	%r153, %r152, 1072693248;
	mov.u32 	%r154, 0;
	mov.b64 	%fd839, {%r154, %r153};
	mul.f64 	%fd1541, %fd838, %fd839;

$L__BB0_90:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r155}, %fd1541;
	}
	and.b32  	%r156, %r155, 2147483647;
	setp.eq.s32 	%p106, %r156, 2146435072;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r157, %temp}, %fd1541;
	}
	setp.eq.s32 	%p107, %r157, 0;
	and.pred  	%p108, %p107, %p106;
	@%p108 bra 	$L__BB0_92;

	// begin inline asm
	fma.rn.f64 	%fd1541, %fd1541, %fd114, %fd1541;
	// end inline asm

$L__BB0_92:
	abs.f64 	%fd1476, %fd105;
	setp.geu.f64 	%p355, %fd1476, 0d0000000000000000;
	setp.neu.f64 	%p109, %fd109, 0d3FF0000000000000;
	or.pred  	%p111, %p355, %p109;
	@%p111 bra 	$L__BB0_99;

	mov.b64 	%rd48, %fd1541;
	xor.b64  	%rd49, %rd48, -9223372036854775808;
	mov.b64 	%fd1541, %rd49;
	bra.uni 	$L__BB0_99;

$L__BB0_157:
	setp.geu.f64 	%p190, %fd165, 0d0000000000000000;
	@%p190 bra 	$L__BB0_159;

	mov.f64 	%fd1053, 0d4000000000000000;
	cvt.rzi.f64.f64 	%fd1054, %fd1053;
	setp.neu.f64 	%p191, %fd1054, 0d4000000000000000;
	mov.f64 	%fd1553, 0dFFF8000000000000;
	@%p191 bra 	$L__BB0_175;

$L__BB0_159:
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r291}, %fd166; 
	}
	// end inline asm
	// begin inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r290, hi}, %fd166; 
	}
	// end inline asm
	bfe.u32 	%r292, %r291, 20, 11;
	setp.ne.s32 	%p192, %r292, 0;
	@%p192 bra 	$L__BB0_161;

	mov.f64 	%fd1059, 0d4350000000000000;
	mul.rn.f64 	%fd1058, %fd166, %fd1059;
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r291}, %fd1058; 
	}
	// end inline asm
	// begin inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r290, hi}, %fd1058; 
	}
	// end inline asm
	bfe.u32 	%r199, %r291, 20, 11;
	add.s32 	%r292, %r199, -54;

$L__BB0_161:
	add.s32 	%r293, %r292, -1023;
	and.b32  	%r202, %r291, -2146435073;
	or.b32  	%r201, %r202, 1072693248;
	// begin inline asm
	mov.b64 	%fd1550, {%r290, %r201};
	// end inline asm
	setp.lt.u32 	%p193, %r201, 1073127583;
	@%p193 bra 	$L__BB0_163;

	// begin inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r203, hi}, %fd1550; 
	}
	// end inline asm
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r204}, %fd1550; 
	}
	// end inline asm
	add.s32 	%r206, %r204, -1048576;
	// begin inline asm
	mov.b64 	%fd1550, {%r203, %r206};
	// end inline asm
	add.s32 	%r293, %r292, -1022;

$L__BB0_163:
	add.f64 	%fd1148, %fd1550, 0d3FF0000000000000;
	mov.f64 	%fd1149, 0d3FF0000000000000;
	rcp.rn.f64 	%fd1150, %fd1148;
	add.f64 	%fd1090, %fd1550, 0dBFF0000000000000;
	mul.rn.f64 	%fd1151, %fd1090, %fd1150;
	add.f64 	%fd1138, %fd1151, %fd1151;
	mul.rn.f64 	%fd1086, %fd1138, %fd1138;
	mov.f64 	%fd1065, 0d3EB0F5FF7D2CAFE2;
	mov.f64 	%fd1067, 0d3ED0F5D241AD3B5A;
	// begin inline asm
	fma.rn.f64 	%fd1064, %fd1065, %fd1086, %fd1067;
	// end inline asm
	mov.f64 	%fd1071, 0d3EF3B20A75488A3F;
	// begin inline asm
	fma.rn.f64 	%fd1068, %fd1064, %fd1086, %fd1071;
	// end inline asm
	mov.f64 	%fd1075, 0d3F1745CDE4FAECD5;
	// begin inline asm
	fma.rn.f64 	%fd1072, %fd1068, %fd1086, %fd1075;
	// end inline asm
	mov.f64 	%fd1079, 0d3F3C71C7258A578B;
	// begin inline asm
	fma.rn.f64 	%fd1076, %fd1072, %fd1086, %fd1079;
	// end inline asm
	mov.f64 	%fd1083, 0d3F6249249242B910;
	// begin inline asm
	fma.rn.f64 	%fd1080, %fd1076, %fd1086, %fd1083;
	// end inline asm
	mov.f64 	%fd1087, 0d3F89999999999DFB;
	// begin inline asm
	fma.rn.f64 	%fd1084, %fd1080, %fd1086, %fd1087;
	// end inline asm
	mul.rn.f64 	%fd1152, %fd1084, %fd1086;
	sub.f64 	%fd1153, %fd1090, %fd1138;
	mov.f64 	%fd1146, 0d4000000000000000;
	mul.rn.f64 	%fd1091, %fd1146, %fd1153;
	neg.f64 	%fd1089, %fd1138;
	// begin inline asm
	fma.rn.f64 	%fd1088, %fd1089, %fd1090, %fd1091;
	// end inline asm
	mul.rn.f64 	%fd1134, %fd1150, %fd1088;
	add.f64 	%fd1154, %fd1152, 0d3FB5555555555555;
	mov.f64 	%fd1155, 0d3FB5555555555555;
	sub.f64 	%fd1156, %fd1155, %fd1154;
	add.f64 	%fd1157, %fd1152, %fd1156;
	add.f64 	%fd1158, %fd1157, 0d0000000000000000;
	add.f64 	%fd1159, %fd1158, 0dBC46A4CB00B9E7B0;
	add.f64 	%fd1101, %fd1154, %fd1159;
	sub.f64 	%fd1160, %fd1154, %fd1101;
	add.f64 	%fd1105, %fd1159, %fd1160;
	mul.rn.f64 	%fd1161, %fd1101, %fd1138;
	neg.f64 	%fd1095, %fd1161;
	// begin inline asm
	fma.rn.f64 	%fd1092, %fd1101, %fd1138, %fd1095;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1096, %fd1105, %fd1134, %fd1092;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1100, %fd1101, %fd1134, %fd1096;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1104, %fd1105, %fd1138, %fd1100;
	// end inline asm
	add.f64 	%fd1117, %fd1161, %fd1104;
	sub.f64 	%fd1162, %fd1161, %fd1117;
	add.f64 	%fd1121, %fd1104, %fd1162;
	mul.rn.f64 	%fd1163, %fd1117, %fd1138;
	neg.f64 	%fd1111, %fd1163;
	// begin inline asm
	fma.rn.f64 	%fd1108, %fd1117, %fd1138, %fd1111;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1112, %fd1121, %fd1134, %fd1108;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1116, %fd1117, %fd1134, %fd1112;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1120, %fd1121, %fd1138, %fd1116;
	// end inline asm
	add.f64 	%fd1133, %fd1163, %fd1120;
	sub.f64 	%fd1164, %fd1163, %fd1133;
	add.f64 	%fd1137, %fd1120, %fd1164;
	mul.rn.f64 	%fd1165, %fd1133, %fd1138;
	neg.f64 	%fd1127, %fd1165;
	// begin inline asm
	fma.rn.f64 	%fd1124, %fd1133, %fd1138, %fd1127;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1128, %fd1137, %fd1134, %fd1124;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1132, %fd1133, %fd1134, %fd1128;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1136, %fd1137, %fd1138, %fd1132;
	// end inline asm
	add.f64 	%fd1166, %fd1165, %fd1136;
	sub.f64 	%fd1167, %fd1165, %fd1166;
	add.f64 	%fd1168, %fd1136, %fd1167;
	add.f64 	%fd1169, %fd1138, %fd1166;
	sub.f64 	%fd1170, %fd1138, %fd1169;
	add.f64 	%fd1171, %fd1166, %fd1170;
	add.f64 	%fd1172, %fd1168, %fd1171;
	add.f64 	%fd1173, %fd1134, %fd1172;
	add.f64 	%fd1174, %fd1169, %fd1173;
	sub.f64 	%fd1175, %fd1169, %fd1174;
	add.f64 	%fd1176, %fd1173, %fd1175;
	cvt.rn.f64.s32 	%fd1177, %r293;
	mov.f64 	%fd1178, 0d3FE62E42FEFA3000;
	mul.rn.f64 	%fd1179, %fd1177, %fd1178;
	mov.f64 	%fd1180, 0d3D53DE6AF278ECE6;
	mul.rn.f64 	%fd1181, %fd1177, %fd1180;
	add.f64 	%fd1182, %fd1179, %fd1174;
	sub.f64 	%fd1183, %fd1179, %fd1182;
	add.f64 	%fd1184, %fd1174, %fd1183;
	add.f64 	%fd1185, %fd1176, %fd1184;
	add.f64 	%fd1186, %fd1181, %fd1185;
	add.f64 	%fd1141, %fd1182, %fd1186;
	sub.f64 	%fd1187, %fd1182, %fd1141;
	add.f64 	%fd1145, %fd1186, %fd1187;
	mul.rn.f64 	%fd1188, %fd1141, %fd1146;
	neg.f64 	%fd1143, %fd1188;
	// begin inline asm
	fma.rn.f64 	%fd1140, %fd1141, %fd1146, %fd1143;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1144, %fd1145, %fd1146, %fd1140;
	// end inline asm
	add.f64 	%fd172, %fd1188, %fd1144;
	sub.f64 	%fd1189, %fd1188, %fd172;
	add.f64 	%fd173, %fd1144, %fd1189;
	mov.f64 	%fd1190, 0d4338000000000000;
	mov.f64 	%fd1191, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd1192, %fd172, %fd1191, %fd1190;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r63, %temp}, %fd1192;
	}
	mov.f64 	%fd1193, 0dC338000000000000;
	add.rn.f64 	%fd1194, %fd1192, %fd1193;
	mov.f64 	%fd1195, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd1196, %fd1194, %fd1195, %fd172;
	mov.f64 	%fd1197, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd1198, %fd1194, %fd1197, %fd1196;
	mov.f64 	%fd1199, 0d3E928AF3FCA213EA;
	mov.f64 	%fd1200, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd1201, %fd1200, %fd1198, %fd1199;
	mov.f64 	%fd1202, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd1203, %fd1201, %fd1198, %fd1202;
	mov.f64 	%fd1204, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd1205, %fd1203, %fd1198, %fd1204;
	mov.f64 	%fd1206, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd1207, %fd1205, %fd1198, %fd1206;
	mov.f64 	%fd1208, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd1209, %fd1207, %fd1198, %fd1208;
	mov.f64 	%fd1210, 0d3F81111111122322;
	fma.rn.f64 	%fd1211, %fd1209, %fd1198, %fd1210;
	mov.f64 	%fd1212, 0d3FA55555555502A1;
	fma.rn.f64 	%fd1213, %fd1211, %fd1198, %fd1212;
	mov.f64 	%fd1214, 0d3FC5555555555511;
	fma.rn.f64 	%fd1215, %fd1213, %fd1198, %fd1214;
	mov.f64 	%fd1216, 0d3FE000000000000B;
	fma.rn.f64 	%fd1217, %fd1215, %fd1198, %fd1216;
	fma.rn.f64 	%fd1218, %fd1217, %fd1198, %fd1149;
	fma.rn.f64 	%fd1219, %fd1218, %fd1198, %fd1149;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r64, %temp}, %fd1219;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r65}, %fd1219;
	}
	shl.b32 	%r207, %r63, 20;
	add.s32 	%r208, %r65, %r207;
	mov.b64 	%fd1553, {%r64, %r208};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r209}, %fd172;
	}
	mov.b32 	%f11, %r209;
	abs.f32 	%f3, %f11;
	setp.lt.f32 	%p194, %f3, 0f4086232B;
	@%p194 bra 	$L__BB0_166;

	setp.lt.f64 	%p195, %fd172, 0d0000000000000000;
	add.f64 	%fd1220, %fd172, 0d7FF0000000000000;
	selp.f64 	%fd1553, 0d0000000000000000, %fd1220, %p195;
	setp.geu.f32 	%p196, %f3, 0f40874800;
	@%p196 bra 	$L__BB0_166;

	mov.f64 	%fd1465, 0d4338000000000000;
	mov.f64 	%fd1464, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd1463, %fd172, %fd1464, %fd1465;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r269, %temp}, %fd1463;
	}
	shr.u32 	%r210, %r269, 31;
	add.s32 	%r211, %r269, %r210;
	shr.s32 	%r212, %r211, 1;
	shl.b32 	%r213, %r212, 20;
	add.s32 	%r214, %r65, %r213;
	mov.b64 	%fd1221, {%r64, %r214};
	sub.s32 	%r215, %r269, %r212;
	shl.b32 	%r216, %r215, 20;
	add.s32 	%r217, %r216, 1072693248;
	mov.u32 	%r218, 0;
	mov.b64 	%fd1222, {%r218, %r217};
	mul.f64 	%fd1553, %fd1221, %fd1222;

$L__BB0_166:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r219}, %fd1553;
	}
	and.b32  	%r220, %r219, 2147483647;
	setp.eq.s32 	%p197, %r220, 2146435072;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r221, %temp}, %fd1553;
	}
	setp.eq.s32 	%p198, %r221, 0;
	and.pred  	%p199, %p198, %p197;
	@%p199 bra 	$L__BB0_168;

	// begin inline asm
	fma.rn.f64 	%fd1553, %fd1553, %fd173, %fd1553;
	// end inline asm

$L__BB0_168:
	abs.f64 	%fd1466, %fd164;
	setp.geu.f64 	%p353, %fd1466, 0d0000000000000000;
	setp.neu.f64 	%p200, %fd168, 0d3FF0000000000000;
	or.pred  	%p202, %p353, %p200;
	@%p202 bra 	$L__BB0_175;

	mov.b64 	%rd55, %fd1553;
	xor.b64  	%rd56, %rd55, -9223372036854775808;
	mov.b64 	%fd1553, %rd56;
	bra.uni 	$L__BB0_175;

$L__BB0_109:
	abs.f64 	%fd1482, %fd105;
	setp.geu.f64 	%p129, %fd1482, 0d0000000000000000;
	@%p129 bra 	$L__BB0_111;

	cvt.rzi.f64.f64 	%fd858, %fd126;
	setp.neu.f64 	%p130, %fd858, %fd126;
	mov.f64 	%fd1546, 0dFFF8000000000000;
	@%p130 bra 	$L__BB0_130;

$L__BB0_111:
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r287}, %fd127; 
	}
	// end inline asm
	// begin inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r286, hi}, %fd127; 
	}
	// end inline asm
	bfe.u32 	%r288, %r287, 20, 11;
	setp.ne.s32 	%p131, %r288, 0;
	@%p131 bra 	$L__BB0_113;

	mov.f64 	%fd863, 0d4350000000000000;
	mul.rn.f64 	%fd862, %fd127, %fd863;
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r287}, %fd862; 
	}
	// end inline asm
	// begin inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r286, hi}, %fd862; 
	}
	// end inline asm
	bfe.u32 	%r166, %r287, 20, 11;
	add.s32 	%r288, %r166, -54;

$L__BB0_113:
	and.b32  	%r169, %r287, -2146435073;
	or.b32  	%r168, %r169, 1072693248;
	// begin inline asm
	mov.b64 	%fd1542, {%r286, %r168};
	// end inline asm
	setp.lt.u32 	%p132, %r168, 1073127583;
	add.s32 	%r289, %r288, -1023;
	@%p132 bra 	$L__BB0_115;

	// begin inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r170, hi}, %fd1542; 
	}
	// end inline asm
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r171}, %fd1542; 
	}
	// end inline asm
	add.s32 	%r173, %r171, -1048576;
	// begin inline asm
	mov.b64 	%fd1542, {%r170, %r173};
	// end inline asm
	add.s32 	%r289, %r288, -1022;

$L__BB0_115:
	add.f64 	%fd944, %fd1542, 0d3FF0000000000000;
	rcp.rn.f64 	%fd945, %fd944;
	add.f64 	%fd894, %fd1542, 0dBFF0000000000000;
	mul.rn.f64 	%fd946, %fd894, %fd945;
	add.f64 	%fd942, %fd946, %fd946;
	mul.rn.f64 	%fd890, %fd942, %fd942;
	mov.f64 	%fd869, 0d3EB0F5FF7D2CAFE2;
	mov.f64 	%fd871, 0d3ED0F5D241AD3B5A;
	// begin inline asm
	fma.rn.f64 	%fd868, %fd869, %fd890, %fd871;
	// end inline asm
	mov.f64 	%fd875, 0d3EF3B20A75488A3F;
	// begin inline asm
	fma.rn.f64 	%fd872, %fd868, %fd890, %fd875;
	// end inline asm
	mov.f64 	%fd879, 0d3F1745CDE4FAECD5;
	// begin inline asm
	fma.rn.f64 	%fd876, %fd872, %fd890, %fd879;
	// end inline asm
	mov.f64 	%fd883, 0d3F3C71C7258A578B;
	// begin inline asm
	fma.rn.f64 	%fd880, %fd876, %fd890, %fd883;
	// end inline asm
	mov.f64 	%fd887, 0d3F6249249242B910;
	// begin inline asm
	fma.rn.f64 	%fd884, %fd880, %fd890, %fd887;
	// end inline asm
	mov.f64 	%fd891, 0d3F89999999999DFB;
	// begin inline asm
	fma.rn.f64 	%fd888, %fd884, %fd890, %fd891;
	// end inline asm
	mul.rn.f64 	%fd947, %fd888, %fd890;
	sub.f64 	%fd948, %fd894, %fd942;
	mov.f64 	%fd949, 0d4000000000000000;
	mul.rn.f64 	%fd895, %fd949, %fd948;
	neg.f64 	%fd893, %fd942;
	// begin inline asm
	fma.rn.f64 	%fd892, %fd893, %fd894, %fd895;
	// end inline asm
	mul.rn.f64 	%fd938, %fd945, %fd892;
	add.f64 	%fd950, %fd947, 0d3FB5555555555555;
	mov.f64 	%fd951, 0d3FB5555555555555;
	sub.f64 	%fd952, %fd951, %fd950;
	add.f64 	%fd953, %fd947, %fd952;
	add.f64 	%fd954, %fd953, 0d0000000000000000;
	add.f64 	%fd955, %fd954, 0dBC46A4CB00B9E7B0;
	add.f64 	%fd905, %fd950, %fd955;
	sub.f64 	%fd956, %fd950, %fd905;
	add.f64 	%fd909, %fd955, %fd956;
	mul.rn.f64 	%fd957, %fd905, %fd942;
	neg.f64 	%fd899, %fd957;
	// begin inline asm
	fma.rn.f64 	%fd896, %fd905, %fd942, %fd899;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd900, %fd909, %fd938, %fd896;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd904, %fd905, %fd938, %fd900;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd908, %fd909, %fd942, %fd904;
	// end inline asm
	add.f64 	%fd921, %fd957, %fd908;
	sub.f64 	%fd958, %fd957, %fd921;
	add.f64 	%fd925, %fd908, %fd958;
	mul.rn.f64 	%fd959, %fd921, %fd942;
	neg.f64 	%fd915, %fd959;
	// begin inline asm
	fma.rn.f64 	%fd912, %fd921, %fd942, %fd915;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd916, %fd925, %fd938, %fd912;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd920, %fd921, %fd938, %fd916;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd924, %fd925, %fd942, %fd920;
	// end inline asm
	add.f64 	%fd937, %fd959, %fd924;
	sub.f64 	%fd960, %fd959, %fd937;
	add.f64 	%fd941, %fd924, %fd960;
	mul.rn.f64 	%fd961, %fd937, %fd942;
	neg.f64 	%fd931, %fd961;
	// begin inline asm
	fma.rn.f64 	%fd928, %fd937, %fd942, %fd931;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd932, %fd941, %fd938, %fd928;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd936, %fd937, %fd938, %fd932;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd940, %fd941, %fd942, %fd936;
	// end inline asm
	add.f64 	%fd962, %fd961, %fd940;
	sub.f64 	%fd963, %fd961, %fd962;
	add.f64 	%fd964, %fd940, %fd963;
	add.f64 	%fd965, %fd942, %fd962;
	sub.f64 	%fd966, %fd942, %fd965;
	add.f64 	%fd967, %fd962, %fd966;
	add.f64 	%fd968, %fd964, %fd967;
	add.f64 	%fd969, %fd938, %fd968;
	add.f64 	%fd970, %fd965, %fd969;
	sub.f64 	%fd971, %fd965, %fd970;
	add.f64 	%fd972, %fd969, %fd971;
	cvt.rn.f64.s32 	%fd973, %r289;
	mov.f64 	%fd974, 0d3FE62E42FEFA3000;
	mul.rn.f64 	%fd975, %fd973, %fd974;
	mov.f64 	%fd976, 0d3D53DE6AF278ECE6;
	mul.rn.f64 	%fd977, %fd973, %fd976;
	add.f64 	%fd135, %fd975, %fd970;
	sub.f64 	%fd978, %fd975, %fd135;
	add.f64 	%fd979, %fd970, %fd978;
	add.f64 	%fd980, %fd972, %fd979;
	add.f64 	%fd136, %fd977, %fd980;
	setp.leu.f64 	%p133, %fd128, 0d7F0D2A1BE4048F90;
	@%p133 bra 	$L__BB0_117;

	mov.f64 	%fd981, 0d3F20000000000000;
	mul.rn.f64 	%fd126, %fd126, %fd981;

$L__BB0_117:
	add.f64 	%fd983, %fd135, %fd136;
	mul.rn.f64 	%fd990, %fd983, %fd126;
	neg.f64 	%fd985, %fd990;
	// begin inline asm
	fma.rn.f64 	%fd982, %fd983, %fd126, %fd985;
	// end inline asm
	sub.f64 	%fd991, %fd135, %fd983;
	add.f64 	%fd987, %fd136, %fd991;
	// begin inline asm
	fma.rn.f64 	%fd986, %fd987, %fd126, %fd982;
	// end inline asm
	add.f64 	%fd139, %fd990, %fd986;
	sub.f64 	%fd992, %fd990, %fd139;
	add.f64 	%fd140, %fd986, %fd992;
	mov.f64 	%fd993, 0d4338000000000000;
	mov.f64 	%fd994, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd995, %fd139, %fd994, %fd993;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r48, %temp}, %fd995;
	}
	mov.f64 	%fd996, 0dC338000000000000;
	add.rn.f64 	%fd997, %fd995, %fd996;
	mov.f64 	%fd998, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd999, %fd997, %fd998, %fd139;
	mov.f64 	%fd1000, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd1001, %fd997, %fd1000, %fd999;
	mov.f64 	%fd1002, 0d3E928AF3FCA213EA;
	mov.f64 	%fd1003, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd1004, %fd1003, %fd1001, %fd1002;
	mov.f64 	%fd1005, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd1006, %fd1004, %fd1001, %fd1005;
	mov.f64 	%fd1007, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd1008, %fd1006, %fd1001, %fd1007;
	mov.f64 	%fd1009, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd1010, %fd1008, %fd1001, %fd1009;
	mov.f64 	%fd1011, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd1012, %fd1010, %fd1001, %fd1011;
	mov.f64 	%fd1013, 0d3F81111111122322;
	fma.rn.f64 	%fd1014, %fd1012, %fd1001, %fd1013;
	mov.f64 	%fd1015, 0d3FA55555555502A1;
	fma.rn.f64 	%fd1016, %fd1014, %fd1001, %fd1015;
	mov.f64 	%fd1017, 0d3FC5555555555511;
	fma.rn.f64 	%fd1018, %fd1016, %fd1001, %fd1017;
	mov.f64 	%fd1019, 0d3FE000000000000B;
	fma.rn.f64 	%fd1020, %fd1018, %fd1001, %fd1019;
	mov.f64 	%fd1021, 0d3FF0000000000000;
	fma.rn.f64 	%fd1022, %fd1020, %fd1001, %fd1021;
	fma.rn.f64 	%fd1023, %fd1022, %fd1001, %fd1021;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r49, %temp}, %fd1023;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r50}, %fd1023;
	}
	shl.b32 	%r174, %r48, 20;
	add.s32 	%r175, %r50, %r174;
	mov.b64 	%fd1546, {%r49, %r175};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r176}, %fd139;
	}
	mov.b32 	%f10, %r176;
	abs.f32 	%f2, %f10;
	setp.lt.f32 	%p134, %f2, 0f4086232B;
	@%p134 bra 	$L__BB0_120;

	setp.lt.f64 	%p135, %fd139, 0d0000000000000000;
	add.f64 	%fd1024, %fd139, 0d7FF0000000000000;
	selp.f64 	%fd1546, 0d0000000000000000, %fd1024, %p135;
	setp.geu.f32 	%p136, %f2, 0f40874800;
	@%p136 bra 	$L__BB0_120;

	shr.u32 	%r177, %r48, 31;
	add.s32 	%r178, %r48, %r177;
	shr.s32 	%r179, %r178, 1;
	shl.b32 	%r180, %r179, 20;
	add.s32 	%r181, %r50, %r180;
	mov.b64 	%fd1025, {%r49, %r181};
	sub.s32 	%r182, %r48, %r179;
	shl.b32 	%r183, %r182, 20;
	add.s32 	%r184, %r183, 1072693248;
	mov.u32 	%r185, 0;
	mov.b64 	%fd1026, {%r185, %r184};
	mul.f64 	%fd1546, %fd1025, %fd1026;

$L__BB0_120:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r186}, %fd1546;
	}
	and.b32  	%r187, %r186, 2147483647;
	setp.eq.s32 	%p137, %r187, 2146435072;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r188, %temp}, %fd1546;
	}
	setp.eq.s32 	%p138, %r188, 0;
	and.pred  	%p139, %p138, %p137;
	@%p139 bra 	$L__BB0_122;

	// begin inline asm
	fma.rn.f64 	%fd1546, %fd1546, %fd140, %fd1546;
	// end inline asm

$L__BB0_122:
	setp.neu.f64 	%p140, %fd131, 0d3FF0000000000000;
	or.pred  	%p142, %p129, %p140;
	@%p142 bra 	$L__BB0_130;

	mov.b64 	%rd50, %fd1546;
	xor.b64  	%rd51, %rd50, -9223372036854775808;
	mov.b64 	%fd1546, %rd51;
	bra.uni 	$L__BB0_130;

$L__BB0_185:
	abs.f64 	%fd1473, %fd164;
	setp.geu.f64 	%p220, %fd1473, 0d0000000000000000;
	@%p220 bra 	$L__BB0_187;

	cvt.rzi.f64.f64 	%fd1241, %fd185;
	setp.neu.f64 	%p221, %fd1241, %fd185;
	mov.f64 	%fd1558, 0dFFF8000000000000;
	@%p221 bra 	$L__BB0_206;

$L__BB0_187:
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r295}, %fd186; 
	}
	// end inline asm
	// begin inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r294, hi}, %fd186; 
	}
	// end inline asm
	bfe.u32 	%r296, %r295, 20, 11;
	setp.ne.s32 	%p222, %r296, 0;
	@%p222 bra 	$L__BB0_189;

	mov.f64 	%fd1246, 0d4350000000000000;
	mul.rn.f64 	%fd1245, %fd186, %fd1246;
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r295}, %fd1245; 
	}
	// end inline asm
	// begin inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r294, hi}, %fd1245; 
	}
	// end inline asm
	bfe.u32 	%r230, %r295, 20, 11;
	add.s32 	%r296, %r230, -54;

$L__BB0_189:
	add.s32 	%r297, %r296, -1023;
	and.b32  	%r233, %r295, -2146435073;
	or.b32  	%r232, %r233, 1072693248;
	// begin inline asm
	mov.b64 	%fd1554, {%r294, %r232};
	// end inline asm
	setp.lt.u32 	%p223, %r232, 1073127583;
	@%p223 bra 	$L__BB0_191;

	// begin inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r234, hi}, %fd1554; 
	}
	// end inline asm
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r235}, %fd1554; 
	}
	// end inline asm
	add.s32 	%r237, %r235, -1048576;
	// begin inline asm
	mov.b64 	%fd1554, {%r234, %r237};
	// end inline asm
	add.s32 	%r297, %r296, -1022;

$L__BB0_191:
	add.f64 	%fd1327, %fd1554, 0d3FF0000000000000;
	rcp.rn.f64 	%fd1328, %fd1327;
	add.f64 	%fd1277, %fd1554, 0dBFF0000000000000;
	mul.rn.f64 	%fd1329, %fd1277, %fd1328;
	add.f64 	%fd1325, %fd1329, %fd1329;
	mul.rn.f64 	%fd1273, %fd1325, %fd1325;
	mov.f64 	%fd1252, 0d3EB0F5FF7D2CAFE2;
	mov.f64 	%fd1254, 0d3ED0F5D241AD3B5A;
	// begin inline asm
	fma.rn.f64 	%fd1251, %fd1252, %fd1273, %fd1254;
	// end inline asm
	mov.f64 	%fd1258, 0d3EF3B20A75488A3F;
	// begin inline asm
	fma.rn.f64 	%fd1255, %fd1251, %fd1273, %fd1258;
	// end inline asm
	mov.f64 	%fd1262, 0d3F1745CDE4FAECD5;
	// begin inline asm
	fma.rn.f64 	%fd1259, %fd1255, %fd1273, %fd1262;
	// end inline asm
	mov.f64 	%fd1266, 0d3F3C71C7258A578B;
	// begin inline asm
	fma.rn.f64 	%fd1263, %fd1259, %fd1273, %fd1266;
	// end inline asm
	mov.f64 	%fd1270, 0d3F6249249242B910;
	// begin inline asm
	fma.rn.f64 	%fd1267, %fd1263, %fd1273, %fd1270;
	// end inline asm
	mov.f64 	%fd1274, 0d3F89999999999DFB;
	// begin inline asm
	fma.rn.f64 	%fd1271, %fd1267, %fd1273, %fd1274;
	// end inline asm
	mul.rn.f64 	%fd1330, %fd1271, %fd1273;
	sub.f64 	%fd1331, %fd1277, %fd1325;
	mov.f64 	%fd1332, 0d4000000000000000;
	mul.rn.f64 	%fd1278, %fd1332, %fd1331;
	neg.f64 	%fd1276, %fd1325;
	// begin inline asm
	fma.rn.f64 	%fd1275, %fd1276, %fd1277, %fd1278;
	// end inline asm
	mul.rn.f64 	%fd1321, %fd1328, %fd1275;
	add.f64 	%fd1333, %fd1330, 0d3FB5555555555555;
	mov.f64 	%fd1334, 0d3FB5555555555555;
	sub.f64 	%fd1335, %fd1334, %fd1333;
	add.f64 	%fd1336, %fd1330, %fd1335;
	add.f64 	%fd1337, %fd1336, 0d0000000000000000;
	add.f64 	%fd1338, %fd1337, 0dBC46A4CB00B9E7B0;
	add.f64 	%fd1288, %fd1333, %fd1338;
	sub.f64 	%fd1339, %fd1333, %fd1288;
	add.f64 	%fd1292, %fd1338, %fd1339;
	mul.rn.f64 	%fd1340, %fd1288, %fd1325;
	neg.f64 	%fd1282, %fd1340;
	// begin inline asm
	fma.rn.f64 	%fd1279, %fd1288, %fd1325, %fd1282;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1283, %fd1292, %fd1321, %fd1279;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1287, %fd1288, %fd1321, %fd1283;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1291, %fd1292, %fd1325, %fd1287;
	// end inline asm
	add.f64 	%fd1304, %fd1340, %fd1291;
	sub.f64 	%fd1341, %fd1340, %fd1304;
	add.f64 	%fd1308, %fd1291, %fd1341;
	mul.rn.f64 	%fd1342, %fd1304, %fd1325;
	neg.f64 	%fd1298, %fd1342;
	// begin inline asm
	fma.rn.f64 	%fd1295, %fd1304, %fd1325, %fd1298;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1299, %fd1308, %fd1321, %fd1295;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1303, %fd1304, %fd1321, %fd1299;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1307, %fd1308, %fd1325, %fd1303;
	// end inline asm
	add.f64 	%fd1320, %fd1342, %fd1307;
	sub.f64 	%fd1343, %fd1342, %fd1320;
	add.f64 	%fd1324, %fd1307, %fd1343;
	mul.rn.f64 	%fd1344, %fd1320, %fd1325;
	neg.f64 	%fd1314, %fd1344;
	// begin inline asm
	fma.rn.f64 	%fd1311, %fd1320, %fd1325, %fd1314;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1315, %fd1324, %fd1321, %fd1311;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1319, %fd1320, %fd1321, %fd1315;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1323, %fd1324, %fd1325, %fd1319;
	// end inline asm
	add.f64 	%fd1345, %fd1344, %fd1323;
	sub.f64 	%fd1346, %fd1344, %fd1345;
	add.f64 	%fd1347, %fd1323, %fd1346;
	add.f64 	%fd1348, %fd1325, %fd1345;
	sub.f64 	%fd1349, %fd1325, %fd1348;
	add.f64 	%fd1350, %fd1345, %fd1349;
	add.f64 	%fd1351, %fd1347, %fd1350;
	add.f64 	%fd1352, %fd1321, %fd1351;
	add.f64 	%fd1353, %fd1348, %fd1352;
	sub.f64 	%fd1354, %fd1348, %fd1353;
	add.f64 	%fd1355, %fd1352, %fd1354;
	cvt.rn.f64.s32 	%fd1356, %r297;
	mov.f64 	%fd1357, 0d3FE62E42FEFA3000;
	mul.rn.f64 	%fd1358, %fd1356, %fd1357;
	mov.f64 	%fd1359, 0d3D53DE6AF278ECE6;
	mul.rn.f64 	%fd1360, %fd1356, %fd1359;
	add.f64 	%fd194, %fd1358, %fd1353;
	sub.f64 	%fd1361, %fd1358, %fd194;
	add.f64 	%fd1362, %fd1353, %fd1361;
	add.f64 	%fd1363, %fd1355, %fd1362;
	add.f64 	%fd195, %fd1360, %fd1363;
	setp.leu.f64 	%p224, %fd187, 0d7F0D2A1BE4048F90;
	@%p224 bra 	$L__BB0_193;

	mov.f64 	%fd1364, 0d3F20000000000000;
	mul.rn.f64 	%fd185, %fd185, %fd1364;

$L__BB0_193:
	add.f64 	%fd1366, %fd194, %fd195;
	mul.rn.f64 	%fd1373, %fd1366, %fd185;
	neg.f64 	%fd1368, %fd1373;
	// begin inline asm
	fma.rn.f64 	%fd1365, %fd1366, %fd185, %fd1368;
	// end inline asm
	sub.f64 	%fd1374, %fd194, %fd1366;
	add.f64 	%fd1370, %fd195, %fd1374;
	// begin inline asm
	fma.rn.f64 	%fd1369, %fd1370, %fd185, %fd1365;
	// end inline asm
	add.f64 	%fd198, %fd1373, %fd1369;
	sub.f64 	%fd1375, %fd1373, %fd198;
	add.f64 	%fd199, %fd1369, %fd1375;
	mov.f64 	%fd1376, 0d4338000000000000;
	mov.f64 	%fd1377, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd1378, %fd198, %fd1377, %fd1376;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r78, %temp}, %fd1378;
	}
	mov.f64 	%fd1379, 0dC338000000000000;
	add.rn.f64 	%fd1380, %fd1378, %fd1379;
	mov.f64 	%fd1381, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd1382, %fd1380, %fd1381, %fd198;
	mov.f64 	%fd1383, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd1384, %fd1380, %fd1383, %fd1382;
	mov.f64 	%fd1385, 0d3E928AF3FCA213EA;
	mov.f64 	%fd1386, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd1387, %fd1386, %fd1384, %fd1385;
	mov.f64 	%fd1388, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd1389, %fd1387, %fd1384, %fd1388;
	mov.f64 	%fd1390, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd1391, %fd1389, %fd1384, %fd1390;
	mov.f64 	%fd1392, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd1393, %fd1391, %fd1384, %fd1392;
	mov.f64 	%fd1394, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd1395, %fd1393, %fd1384, %fd1394;
	mov.f64 	%fd1396, 0d3F81111111122322;
	fma.rn.f64 	%fd1397, %fd1395, %fd1384, %fd1396;
	mov.f64 	%fd1398, 0d3FA55555555502A1;
	fma.rn.f64 	%fd1399, %fd1397, %fd1384, %fd1398;
	mov.f64 	%fd1400, 0d3FC5555555555511;
	fma.rn.f64 	%fd1401, %fd1399, %fd1384, %fd1400;
	mov.f64 	%fd1402, 0d3FE000000000000B;
	fma.rn.f64 	%fd1403, %fd1401, %fd1384, %fd1402;
	mov.f64 	%fd1404, 0d3FF0000000000000;
	fma.rn.f64 	%fd1405, %fd1403, %fd1384, %fd1404;
	fma.rn.f64 	%fd1406, %fd1405, %fd1384, %fd1404;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r79, %temp}, %fd1406;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r80}, %fd1406;
	}
	shl.b32 	%r238, %r78, 20;
	add.s32 	%r239, %r80, %r238;
	mov.b64 	%fd1558, {%r79, %r239};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r240}, %fd198;
	}
	mov.b32 	%f12, %r240;
	abs.f32 	%f4, %f12;
	setp.lt.f32 	%p225, %f4, 0f4086232B;
	@%p225 bra 	$L__BB0_196;

	setp.lt.f64 	%p226, %fd198, 0d0000000000000000;
	add.f64 	%fd1407, %fd198, 0d7FF0000000000000;
	selp.f64 	%fd1558, 0d0000000000000000, %fd1407, %p226;
	setp.geu.f32 	%p227, %f4, 0f40874800;
	@%p227 bra 	$L__BB0_196;

	shr.u32 	%r241, %r78, 31;
	add.s32 	%r242, %r78, %r241;
	shr.s32 	%r243, %r242, 1;
	shl.b32 	%r244, %r243, 20;
	add.s32 	%r245, %r80, %r244;
	mov.b64 	%fd1408, {%r79, %r245};
	sub.s32 	%r246, %r78, %r243;
	shl.b32 	%r247, %r246, 20;
	add.s32 	%r248, %r247, 1072693248;
	mov.u32 	%r249, 0;
	mov.b64 	%fd1409, {%r249, %r248};
	mul.f64 	%fd1558, %fd1408, %fd1409;

$L__BB0_196:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r250}, %fd1558;
	}
	and.b32  	%r251, %r250, 2147483647;
	setp.eq.s32 	%p228, %r251, 2146435072;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r252, %temp}, %fd1558;
	}
	setp.eq.s32 	%p229, %r252, 0;
	and.pred  	%p230, %p229, %p228;
	@%p230 bra 	$L__BB0_198;

	// begin inline asm
	fma.rn.f64 	%fd1558, %fd1558, %fd199, %fd1558;
	// end inline asm

$L__BB0_198:
	setp.neu.f64 	%p231, %fd190, 0d3FF0000000000000;
	or.pred  	%p233, %p220, %p231;
	@%p233 bra 	$L__BB0_206;

	mov.b64 	%rd57, %fd1558;
	xor.b64  	%rd58, %rd57, -9223372036854775808;
	mov.b64 	%fd1558, %rd58;
	bra.uni 	$L__BB0_206;

}

  